{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'2AIO_1_A'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 265\u001b[0m\n\u001b[1;32m    259\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m [(encode_sequence(seqs_train\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSEQUENCE\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m    260\u001b[0m                   protein_data[seqs_train\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPDB_ID\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[1;32m    261\u001b[0m                   [sec_struct_mapping[char] \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m labels_train\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSEC_STRUCT\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m    262\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seqs_train))]\n\u001b[1;32m    263\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[0;32m--> 265\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m [(seqs_test\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSEQUENCE\u001b[39m\u001b[38;5;124m'\u001b[39m], protein_data[seqs_test\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPDB_ID\u001b[39m\u001b[38;5;124m'\u001b[39m]]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    266\u001b[0m                 \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seqs_test))]\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Define the model, criterion, and optimizer\u001b[39;00m\n\u001b[1;32m    270\u001b[0m model \u001b[38;5;241m=\u001b[39m FullyConvolutionalProteinModel()\n",
      "Cell \u001b[0;32mIn[3], line 265\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m [(encode_sequence(seqs_train\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSEQUENCE\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m    260\u001b[0m                   protein_data[seqs_train\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPDB_ID\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[1;32m    261\u001b[0m                   [sec_struct_mapping[char] \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m labels_train\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSEC_STRUCT\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m    262\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seqs_train))]\n\u001b[1;32m    263\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[0;32m--> 265\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m [(seqs_test\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSEQUENCE\u001b[39m\u001b[38;5;124m'\u001b[39m], protein_data[seqs_test\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPDB_ID\u001b[39m\u001b[38;5;124m'\u001b[39m]]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    266\u001b[0m                 \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seqs_test))]\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Define the model, criterion, and optimizer\u001b[39;00m\n\u001b[1;32m    270\u001b[0m model \u001b[38;5;241m=\u001b[39m FullyConvolutionalProteinModel()\n",
      "\u001b[0;31mKeyError\u001b[0m: '2AIO_1_A'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Define store file paths\n",
    "DATA_PATH = \"./data/\"\n",
    "labels_train_path = DATA_PATH + \"labels_train.csv\"\n",
    "sample_path = DATA_PATH + \"sample.csv\"\n",
    "seqs_test_path = DATA_PATH + \"seqs_test.csv\"\n",
    "seqs_train_path = DATA_PATH + \"seqs_train.csv\"\n",
    "train_path = DATA_PATH + \"train\"\n",
    "test_path = DATA_PATH + \"test\"\n",
    "\n",
    "# Define a mapping from amino acid characters to integers\n",
    "amino_acid_mapping = {\n",
    "    'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4,\n",
    "    'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9,\n",
    "    'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14,\n",
    "    'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19,\n",
    "    'X': 20,  # Typically used for unknown amino acids\n",
    "    'B': 21,  # Asparagine or Aspartic acid\n",
    "    'Z': 22,  # Glutamine or Glutamic acid\n",
    "    'J': 23,  # Leucine or Isoleucine\n",
    "    '-': 24,  # Gap or padding\n",
    "}\n",
    "\n",
    "sec_struct_mapping = {'H': 0, 'E': 1, 'C': 2}  # Add more mappings if there are more labels\n",
    "\n",
    "\n",
    "def encode_sequence(sequence):\n",
    "    # Convert each amino acid in the sequence to a one-hot encoded vector\n",
    "    encoded_sequence = np.zeros((len(sequence), len(amino_acid_mapping)), dtype=int)\n",
    "    for i, amino_acid in enumerate(sequence):\n",
    "        # Default to 'X' for unknown amino acids\n",
    "        index = amino_acid_mapping.get(amino_acid, amino_acid_mapping['X'])\n",
    "        encoded_sequence[i, index] = 1\n",
    "    return encoded_sequence\n",
    "\n",
    "\n",
    "def normalize_pssm(pssm, normalize_method='min-max'):\n",
    "    # Assuming the first two columns are non-numeric; adjust as necessary based on your actual data format\n",
    "    numeric_columns = pssm[:, 2:]  # Adjust this if your numeric data starts from a different column\n",
    "\n",
    "    # Convert to floats\n",
    "    try:\n",
    "        pssm_numeric = numeric_columns.astype(np.float32)\n",
    "    except ValueError as e:\n",
    "        # Handle or log the error if needed\n",
    "        raise ValueError(f\"Error converting PSSM to float: {e}\")\n",
    "\n",
    "    if normalize_method == 'min-max':\n",
    "        # Min-Max normalization\n",
    "        pssm_min = pssm_numeric.min(axis=0)\n",
    "        pssm_max = pssm_numeric.max(axis=0)\n",
    "        # Ensure no division by zero\n",
    "        pssm_range = np.where(pssm_max - pssm_min == 0, 1, pssm_max - pssm_min)\n",
    "        normalized_pssm = (pssm_numeric - pssm_min) / pssm_range\n",
    "    elif normalize_method == 'z-score':\n",
    "        # Z-Score normalization\n",
    "        pssm_mean = pssm_numeric.mean(axis=0)\n",
    "        pssm_std = pssm_numeric.std(axis=0)\n",
    "        # Avoid division by zero\n",
    "        pssm_std = np.where(pssm_std == 0, 1, pssm_std)\n",
    "        normalized_pssm = (pssm_numeric - pssm_mean) / pssm_std\n",
    "    else:\n",
    "        # If no normalization method provided, return the original PSSM\n",
    "        normalized_pssm = pssm_numeric\n",
    "\n",
    "    return normalized_pssm\n",
    "\n",
    "\n",
    "def collate_fn_without_labels(batch):\n",
    "    id, sequences, pssms = zip(*batch)  # Unzip the batch\n",
    "\n",
    "    # Pad sequences and PSSMs\n",
    "    sequences_padded = pad_sequence([torch.tensor(seq, dtype=torch.float32) for seq in sequences], batch_first=True)\n",
    "    pssms_padded = torch.tensor([pssm for pssm in pssms], dtype=torch.float32)\n",
    "\n",
    "    return id, sequences_padded, pssms_padded\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, pssms, labels_list = zip(*batch)  # Unzip the batch\n",
    "\n",
    "    # Pad sequences and PSSMs\n",
    "    sequences_padded = pad_sequence([torch.tensor(seq, dtype=torch.float32) for seq in sequences], batch_first=True)\n",
    "    pssms_padded = pad_sequence([torch.tensor(pssm, dtype=torch.float32) for pssm in pssms], batch_first=True)\n",
    "\n",
    "    # Handling labels correctly\n",
    "    if labels_list[0] is not None:  # Check if labels exist\n",
    "        labels_padded = pad_sequence([torch.tensor(label, dtype=torch.long) for label in labels_list], batch_first=True)\n",
    "    else:\n",
    "        labels_padded = None\n",
    "\n",
    "    # Create a mask based on the original sequence lengths\n",
    "    mask = [torch.ones(len(label), dtype=torch.uint8) for label in labels_list]\n",
    "    mask_padded = pad_sequence(mask, batch_first=True, padding_value=0)  # Assuming padding_value for labels is 0\n",
    "    return sequences_padded, pssms_padded, labels_padded, mask_padded\n",
    "\n",
    "\n",
    "class FullyConvolutionalProteinModel(nn.Module):\n",
    "    def __init__(self, num_classes=3, input_channels=20):  # 20 for amino acid one-hot, adjust if using PSSM\n",
    "        super(FullyConvolutionalProteinModel, self).__init__()\n",
    "\n",
    "        # Define convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        # Final layer that maps to the number of classes\n",
    "        self.final_conv = nn.Conv1d(in_channels=256, out_channels=num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with activation functions\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Apply final convolutional layer - no activation, as CrossEntropyLoss includes it\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # No softmax here, as nn.CrossEntropyLoss applies it internally.\n",
    "        # Transpose the output to match [batch_size, sequence_length, num_classes]\n",
    "        # This makes it easier to calculate loss later\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_dataloader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "\n",
    "        for sequences, pssms, labels, _ in train_dataloader:\n",
    "            inputs = pssms.permute(0, 2, 1)  # Adjust for PSSM data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.transpose(1, 2), labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs, 2)  # Get the index of the max log-probability\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.numel()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "        epoch_acc = correct_preds / total_preds\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "\n",
    "def validate_model(model, criterion, val_dataloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, pssms, labels, _ in val_dataloader:\n",
    "            inputs = pssms.permute(0, 2, 1)  # Adjust for PSSM data\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.transpose(1, 2), labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 2)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.numel()\n",
    "\n",
    "    val_loss = running_loss / len(val_dataloader.dataset)\n",
    "    val_acc = correct_preds / total_preds\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "\n",
    "def test_model(model, criterion, test_dataloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, pssms, labels, _ in test_dataloader:\n",
    "            inputs = pssms.permute(0, 2, 1)  # Adjust for PSSM data\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.transpose(1, 2), labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 2)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.numel()\n",
    "\n",
    "    test_loss = running_loss / len(test_dataloader.dataset)\n",
    "    test_acc = correct_preds / total_preds\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "\n",
    "def test_model_direct(model, test_seqs, test_pssms, output_file='./submission.csv'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_seqs)):\n",
    "            pdb_id = test_seqs.iloc[i]['PDB_ID']\n",
    "            sequence = test_seqs.iloc[i]['SEQUENCE']\n",
    "            pssm = test_pssms[i]\n",
    "\n",
    "            # Encode and prepare the input tensor; add an extra batch dimension using unsqueeze\n",
    "            input_seq = encode_sequence(sequence)\n",
    "            input_pssm = torch.tensor(normalize_pssm(pssm), dtype=torch.float32).unsqueeze(0).permute(0, 2, 1)\n",
    "\n",
    "            # Make a prediction\n",
    "            outputs = model(input_pssm)\n",
    "            _, predicted = torch.max(outputs, 2)  # Get the index of max log-probability\n",
    "\n",
    "            # Process the predictions\n",
    "            seq_len = pssm.shape[0]  # Assuming pssm is [features, seq_len]\n",
    "            for j in range(seq_len):\n",
    "                residue_id = f\"{pdb_id}_{j + 1}\"  # Construct the ID\n",
    "                structure_label = ['H', 'E', 'C'][predicted[0, j].item()]  # Map numeric predictions to labels\n",
    "                predictions.append([residue_id, structure_label])\n",
    "\n",
    "    # Write predictions to CSV\n",
    "    pd.DataFrame(predictions, columns=['ID', 'STRUCTURE']).to_csv(output_file, index=False)\n",
    "    print(f'Submission file saved to {output_file}')\n",
    "\n",
    "\n",
    "# Load data\n",
    "seqs_train = pd.read_csv(seqs_train_path)\n",
    "labels_train = pd.read_csv(labels_train_path)\n",
    "seqs_test = pd.read_csv(seqs_test_path)\n",
    "\n",
    "# Load protein data from the directory\n",
    "protein_data = {}\n",
    "for filename in os.listdir(train_path):\n",
    "    if filename.endswith(\".csv\"):  # Check if the file is a CSV\n",
    "        protein_id = re.split(r'_train|_test', filename)[0]\n",
    "        protein_data[protein_id] = pd.read_csv(os.path.join(train_path, filename)).values\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = [(encode_sequence(seqs_train.iloc[i]['SEQUENCE']),\n",
    "                  protein_data[seqs_train.iloc[i]['PDB_ID']],\n",
    "                  [sec_struct_mapping[char] for char in labels_train.iloc[i]['SEC_STRUCT']])\n",
    "                 for i in range(len(seqs_train))]\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = [(seqs_test.iloc[i]['SEQUENCE'], protein_data[seqs_test.iloc[i]['PDB_ID']]) for i in\n",
    "                range(len(seqs_test))]\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn_without_labels)\n",
    "\n",
    "# Define the model, criterion, and optimizer\n",
    "model = FullyConvolutionalProteinModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0)\n",
    "num_epochs = 10\n",
    "\n",
    "# Train the model\n",
    "train_model(model, criterion, optimizer, dataloader, num_epochs=num_epochs)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_model(model, criterion, test_loader)\n",
    "\n",
    "# Make predictions on the test data and save to a submission file\n",
    "test_seqs, test_pssms = zip(*test_dataset)\n",
    "test_model_direct(model, pd.DataFrame(test_seqs), test_pssms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
