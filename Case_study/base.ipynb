{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Convolutional PyTorch Model to Predict Protein Secondary Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Your overall goal is to write a Fully Convolutional PyTorch model that can input protein sequence data (often called the Protein Primary Structure ), or additionally using PSSM Profiles to predict the protein secondary structure (H = Helix, E = Extended Sheet, C = Coil symbols).\n",
    "\n",
    "The PDB Database contains the protein structures of over 200,000 proteins. Each has a unique PDB_ID code such as 1A0S (the first one in the training data) which is the structure shown above (sucrose-specific porin of salmonella) which is used to transfer sucrose across the cell membrane of salmonella bacteria which causes food poisoning. The protein has a 3D Structure which shows that most of this protein is extended beta sheet (flat arrows) and coil (random lines).\n",
    "\n",
    "The Data Tab on Kaggle will allow you to browse the available data used for training. You should use this Data Tab to browse through the data so you understand what it is like. You will find a seqs_train.csv file which is a CSV file that gives the PDB_ID (unique identifier) and the SEQUENCE of each protein. You will also find a train.zip file which contains a large collection of 'PDB_ID'_train.csv files containing residue number, amino acid and PSSM profiles for each residue in that particular protein. The labels_train.csv file contains the secondary structure labels for the different training proteins (given as H = Helix, E = Extended Sheet, C = Coil symbols). The seqs_test.csv and test.zip contain similar data for the test sequences for which you need to predict the secondary structure.\n",
    "\n",
    "IN ADDITION - you will also need to submit your Jupyter Notebook that produces these outputs via the Moodle web page.\n",
    "\n",
    "Please see the Moodle course site for further details about this coursework.\n",
    "<br>\n",
    "### Evaluation\n",
    "The evaluation metric is the \"Q3 Accuracy\" which is used for assessing the three states within a protein structure prediction (H = Helix, E = Extended Sheet, C = Coil). <br>\n",
    "\n",
    "### Submission File\n",
    "For each PDB_ID in the test set, you must predict the secondary structure of each residue in that protein. The file should contain a header and have the following format:\n",
    "\n",
    "(So columns give ID consisting of 'PDB_ID', then underscore 'residue number', followed by the predicted secondary structure label of that residue.)\n",
    "\n",
    "ID,STRUCTURE <br>\n",
    "2AIO_1_A_1, C <br>\n",
    "2AIO_1_A_2, C <br>\n",
    "2AIO_1_A_3, C <br>\n",
    "2AIO_1_A_4, H <br>\n",
    "2AIO_1_A_5, H <br>\n",
    "etc. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Constants\n",
    "DATA_PATH = \"./data/\"\n",
    "labels_train_path = DATA_PATH + \"labels_train.csv\"\n",
    "sample_path = DATA_PATH + \"sample.csv\"\n",
    "seqs_test_path = DATA_PATH + \"seqs_test.csv\"\n",
    "seqs_train_path = DATA_PATH + \"seqs_train.csv\"\n",
    "train_path = DATA_PATH + \"train\"\n",
    "test_path = DATA_PATH + \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping from amino acid characters to integers\n",
    "amino_acid_mapping = {\n",
    "    'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4,\n",
    "    'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9,\n",
    "    'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14,\n",
    "    'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19,\n",
    "    'X': 20,  # Typically used for unknown amino acids\n",
    "    'B': 21,  # Asparagine or Aspartic acid\n",
    "    'Z': 22,  # Glutamine or Glutamic acid\n",
    "    'J': 23,  # Leucine or Isoleucine\n",
    "    '-': 24,  # Gap or padding\n",
    "}\n",
    "\n",
    "sec_struct_mapping = {'H': 0, 'E': 1, 'C': 2}  # Add more mappings if there are more labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, csv_file, train_dir, label_file=None, normalize_method='min-max'):\n",
    "\n",
    "        # Load the sequences\n",
    "        self.seqs = pd.read_csv(csv_file)\n",
    "\n",
    "        # Load the protein data from the directory\n",
    "        self.protein_data = {}\n",
    "        for filename in os.listdir(train_dir):\n",
    "            if filename.endswith(\".csv\"):  # Check if the file is a CSV\n",
    "                protein_id = re.split(r'_train|_test', filename)[0]\n",
    "                self.protein_data[protein_id] = pd.read_csv(os.path.join(train_dir, filename))\n",
    "\n",
    "        # Load the labels, if provided\n",
    "        if label_file:\n",
    "            self.labels = pd.read_csv(label_file)\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "        # Amino acid mapping\n",
    "        self.amino_acid_mapping = amino_acid_mapping\n",
    "        self.normalize_method = normalize_method\n",
    "\n",
    "    def encode_sequence(self, sequence):\n",
    "        # Convert each amino acid in the sequence to a one-hot encoded vector\n",
    "        encoded_sequence = np.zeros((len(sequence), len(self.amino_acid_mapping)), dtype=int)\n",
    "        for i, amino_acid in enumerate(sequence):\n",
    "            # Default to 'X' for unknown amino acids\n",
    "            index = self.amino_acid_mapping.get(amino_acid, self.amino_acid_mapping['X'])\n",
    "            encoded_sequence[i, index] = 1\n",
    "        return encoded_sequence\n",
    "\n",
    "    def normalize_pssm(self, pssm):\n",
    "        # Assuming the first two columns are non-numeric; adjust as necessary based on your actual data format\n",
    "        numeric_columns = pssm[:, 2:]  # Adjust this if your numeric data starts from a different column\n",
    "\n",
    "        # Convert to floats\n",
    "        try:\n",
    "            pssm_numeric = numeric_columns.astype(np.float32)\n",
    "        except ValueError as e:\n",
    "            # Handle or log the error if needed\n",
    "            raise ValueError(f\"Error converting PSSM to float: {e}\")\n",
    "\n",
    "        if self.normalize_method == 'min-max':\n",
    "            # Min-Max normalization\n",
    "            pssm_min = pssm_numeric.min(axis=0)\n",
    "            pssm_max = pssm_numeric.max(axis=0)\n",
    "            # Ensure no division by zero\n",
    "            pssm_range = np.where(pssm_max - pssm_min == 0, 1, pssm_max - pssm_min)\n",
    "            normalized_pssm = (pssm_numeric - pssm_min) / pssm_range\n",
    "        elif self.normalize_method == 'z-score':\n",
    "            # Z-Score normalization\n",
    "            pssm_mean = pssm_numeric.mean(axis=0)\n",
    "            pssm_std = pssm_numeric.std(axis=0)\n",
    "            # Avoid division by zero\n",
    "            pssm_std = np.where(pssm_std == 0, 1, pssm_std)\n",
    "            normalized_pssm = (pssm_numeric - pssm_mean) / pssm_std\n",
    "        else:\n",
    "            # If no normalization method provided, return the original PSSM\n",
    "            normalized_pssm = pssm_numeric\n",
    "\n",
    "        return normalized_pssm\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        protein_id = self.seqs.iloc[idx]['PDB_ID']\n",
    "        sequence = self.seqs.iloc[idx]['SEQUENCE']\n",
    "        encoded_sequence = self.encode_sequence(sequence)  # Encode the sequence\n",
    "        pssm = self.protein_data[protein_id].values  # Assuming you will process PSSM separately\n",
    "        normalized_pssm = self.normalize_pssm(pssm)  # Ensure this is uncommented to use normalized PSSM\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label_seq = self.labels.iloc[idx]['SEC_STRUCT']\n",
    "            label_numeric = [sec_struct_mapping[char] for char in label_seq]\n",
    "            label_tensor = torch.tensor(label_numeric, dtype=torch.long)\n",
    "            return (\n",
    "                protein_id,\n",
    "                torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "                torch.tensor(normalized_pssm, dtype=torch.float32),\n",
    "                label_tensor\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            protein_id,\n",
    "            torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "            torch.tensor(normalized_pssm, dtype=torch.float32)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_without_labels(batch):\n",
    "    id, sequences, pssms = zip(*batch)  # Unzip the batch\n",
    "\n",
    "    # Pad sequences and PSSMs\n",
    "    sequences_padded = pad_sequence([seq.clone().detach() for seq in sequences], batch_first=True)\n",
    "    # sequences_padded =  torch.tensor([seq.clone().detach() for seq in sequences])\n",
    "\n",
    "    pssms_padded = torch.tensor(pssms)\n",
    "    # pssms_padded = pad_sequence([pssm.clone().detach() for pssm in pssms], batch_first=True)\n",
    "\n",
    "    return id, sequences_padded, pssms_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    _, sequences, pssms, labels_list = zip(*batch)  # Unzip the batch\n",
    "\n",
    "    # Pad sequences and PSSMs\n",
    "    sequences_padded = pad_sequence([seq.clone().detach() for seq in sequences], batch_first=True)\n",
    "\n",
    "    pssms_padded = pad_sequence([pssm.clone().detach() for pssm in pssms], batch_first=True)\n",
    "\n",
    "    # Handling labels correctly\n",
    "    if labels_list[0] is not None:  # Check if labels exist\n",
    "        labels_padded = pad_sequence([label.clone().detach() for label in labels_list], batch_first=True)\n",
    "\n",
    "    else:\n",
    "        labels_padded = None\n",
    "\n",
    "    # Create a mask based on the original sequence lengths\n",
    "    mask = [torch.ones(len(label), dtype=torch.uint8) for label in labels_list]\n",
    "    mask_padded = pad_sequence(mask, batch_first=True, padding_value=0)  # Assuming padding_value for labels is 0\n",
    "    return sequences_padded, pssms_padded, labels_padded, mask_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConvolutionalProteinModel(nn.Module):\n",
    "    def __init__(self, num_classes=3, input_channels=20):  # 20 for amino acid one-hot, adjust if using PSSM\n",
    "        super(FullyConvolutionalProteinModel, self).__init__()\n",
    "\n",
    "        # Define convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        # Final layer that maps to the number of classes\n",
    "        self.final_conv = nn.Conv1d(in_channels=256, out_channels=num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with activation functions\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Apply final convolutional layer - no activation, as CrossEntropyLoss includes it\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # No softmax here, as nn.CrossEntropyLoss applies it internally.\n",
    "        # Transpose the output to match [batch_size, sequence_length, num_classes]\n",
    "        # This makes it easier to calculate loss later\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_dataloader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "\n",
    "        for sequences, pssms, labels, _ in train_dataloader:\n",
    "            inputs = pssms.permute(0, 2, 1)  # Adjust for PSSM data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.transpose(1, 2), labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs, 2)  # Get the index of the max log-probability\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.numel()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "        epoch_acc = correct_preds / total_preds\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, criterion, val_dataloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, pssms, labels, _ in val_dataloader:\n",
    "            inputs = pssms.permute(0, 2, 1)  # Adjust for PSSM data\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.transpose(1, 2), labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 2)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.numel()\n",
    "\n",
    "    val_loss = running_loss / len(val_dataloader.dataset)\n",
    "    val_acc = correct_preds / total_preds\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion, test_dataloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, pssms, labels, _ in test_dataloader:\n",
    "            inputs = pssms.permute(0, 2, 1)  # Adjust for PSSM data\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.transpose(1, 2), labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 2)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.numel()\n",
    "\n",
    "    test_loss = running_loss / len(test_dataloader.dataset)\n",
    "    test_acc = correct_preds / total_preds\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_direct(model, test_dataset, output_file='./model/submission.csv'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_dataset)):  # Iterate directly over the dataset\n",
    "            pdb_id, _, pssm = test_dataset[i]  # Assuming the dataset returns PDB_ID, sequence, and PSSM\n",
    "\n",
    "            # Prepare the input tensor; add an extra batch dimension using unsqueeze\n",
    "            input_pssm = pssm.unsqueeze(0).permute(0, 2, 1)  # Adjust dimensions to [1, features, seq_len]\n",
    "\n",
    "            # Make a prediction\n",
    "            outputs = model(input_pssm)\n",
    "            _, predicted = torch.max(outputs, 2)  # Get the index of max log-probability\n",
    "\n",
    "            # Process the predictions\n",
    "            seq_len = pssm.shape[0]  # Assuming pssm is [features, seq_len]\n",
    "            for j in range(seq_len):\n",
    "                residue_id = f\"{pdb_id}_{j + 1}\"  # Construct the ID\n",
    "                structure_label = ['H', 'E', 'C'][predicted[0, j].item()]  # Map numeric predictions to labels\n",
    "                predictions.append([residue_id, structure_label])\n",
    "\n",
    "    # Write predictions to CSV\n",
    "    pd.DataFrame(predictions, columns=['ID', 'STRUCTURE']).to_csv(output_file, index=False)\n",
    "    print(f'Submission file saved to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ProteinDataset(csv_file=seqs_train_path, train_dir=train_path, label_file=labels_train_path)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = ProteinDataset(csv_file=seqs_test_path, train_dir=test_path)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn_without_labels)\n",
    "\n",
    "# Splitting train_dataset into train and validation sets (adjust sizes as needed)\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_subset, val_subset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# val_loader = DataLoader(val_subset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# model = FullyConvolutionalProteinModel(hidden_layers=[64, 128, 256, 512, 1024])\n",
    "model = FullyConvolutionalProteinModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.000957)  # Learning rate may need tuning\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0)\n",
    "num_epochs = 10\n",
    "\n",
    "train_model(model, criterion, optimizer, dataloader)\n",
    "# validate_model(model, criterion, val_loader)\n",
    "# test_model_to_csv(model, test_loader)\n",
    "# test_model_to_csv_test(None, test_loader)\n",
    "\n",
    "test_model_direct(model, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
