{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Convolutional PyTorch Model to Predict Protein Secondary Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Your overall goal is to write a Fully Convolutional PyTorch model that can input protein sequence data (often called the Protein Primary Structure ), or additionally using PSSM Profiles to predict the protein secondary structure (H = Helix, E = Extended Sheet, C = Coil symbols).\n",
    "\n",
    "The PDB Database contains the protein structures of over 200,000 proteins. Each has a unique PDB_ID code such as 1A0S (the first one in the training data) which is the structure shown above (sucrose-specific porin of salmonella) which is used to transfer sucrose across the cell membrane of salmonella bacteria which causes food poisoning. The protein has a 3D Structure which shows that most of this protein is extended beta sheet (flat arrows) and coil (random lines).\n",
    "\n",
    "The Data Tab on Kaggle will allow you to browse the available data used for training. You should use this Data Tab to browse through the data so you understand what it is like. You will find a seqs_train.csv file which is a CSV file that gives the PDB_ID (unique identifier) and the SEQUENCE of each protein. You will also find a train.zip file which contains a large collection of 'PDB_ID'_train.csv files containing residue number, amino acid and PSSM profiles for each residue in that particular protein. The labels_train.csv file contains the secondary structure labels for the different training proteins (given as H = Helix, E = Extended Sheet, C = Coil symbols). The seqs_test.csv and test.zip contain similar data for the test sequences for which you need to predict the secondary structure.\n",
    "\n",
    "IN ADDITION - you will also need to submit your Jupyter Notebook that produces these outputs via the Moodle web page.\n",
    "\n",
    "Please see the Moodle course site for further details about this coursework.\n",
    "<br>\n",
    "### Evaluation\n",
    "The evaluation metric is the \"Q3 Accuracy\" which is used for assessing the three states within a protein structure prediction (H = Helix, E = Extended Sheet, C = Coil). <br>\n",
    "\n",
    "### Submission File\n",
    "For each PDB_ID in the test set, you must predict the secondary structure of each residue in that protein. The file should contain a header and have the following format:\n",
    "\n",
    "(So columns give ID consisting of 'PDB_ID', then underscore 'residue number', followed by the predicted secondary structure label of that residue.)\n",
    "\n",
    "ID,STRUCTURE <br>\n",
    "2AIO_1_A_1, C <br>\n",
    "2AIO_1_A_2, C <br>\n",
    "2AIO_1_A_3, C <br>\n",
    "2AIO_1_A_4, H <br>\n",
    "2AIO_1_A_5, H <br>\n",
    "etc. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries and store file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "# Define store file paths\n",
    "DATA_PATH = \"./data/\"\n",
    "labels_train_path = DATA_PATH + \"labels_train.csv\"\n",
    "sample_path = DATA_PATH + \"sample.csv\"\n",
    "seqs_test_path = DATA_PATH + \"seqs_test.csv\"\n",
    "seqs_train_path = DATA_PATH + \"seqs_train.csv\"\n",
    "train_path = DATA_PATH + \"train\"\n",
    "test_path = DATA_PATH + \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a mapping from amino acid characters to integers\n",
    "\n",
    "To enable model training and assessment, these mappings from amino acid characters to integers for encoding are necessary for converting categorical data into numerical representation: \n",
    "- `sec_struct_mapping`: A dictionary mapping secondary structure labels ('H' for Helix, 'E' for Extended Sheet, 'C' for Coil) to integer labels (0, 1, 2 respectively). Additional mappings can be added if there are more labels.\n",
    "- `amino_acid_mapping`: A dictionary mapping amino acid characters to integer labels. Each amino acid is assigned a unique integer, with additional mappings provided for special cases such as unknown amino acids ('X'), ambiguous cases ('B', 'Z', 'J'), and gap or padding ('-').\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping from amino acid characters to integers\n",
    "sec_struct_mapping = {'H': 0, 'E': 1, 'C': 2}  # Add more mappings if there are more labels\n",
    "amino_acid_mapping = {\n",
    "    'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4,\n",
    "    'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9,\n",
    "    'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14,\n",
    "    'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19,\n",
    "    'X': 20,  # Typically used for unknown amino acids\n",
    "    'B': 21,  # Asparagine or Aspartic acid\n",
    "    'Z': 22,  # Glutamine or Glutamic acid\n",
    "    'J': 23,  # Leucine or Isoleucine\n",
    "    '-': 24,  # Gap or padding\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_protein_data(csv_file, train_dir):\n",
    "#     \"\"\"Loads protein data from CSV and directory.\"\"\"\n",
    "#     seqs = pd.read_csv(csv_file)\n",
    "#     protein_data = {}\n",
    "#     for filename in os.listdir(train_dir):\n",
    "#         if filename.endswith(\".csv\"):\n",
    "#             protein_id = re.split(r'_train|_test', filename)[0]\n",
    "#             protein_data[protein_id] = pd.read_csv(os.path.join(train_dir, filename))\n",
    "#     return seqs, protein_data\n",
    "\n",
    "# def load_labels(label_file):\n",
    "#     \"\"\"Loads labels from a CSV file.\"\"\"\n",
    "#     if label_file:\n",
    "#         return pd.read_csv(label_file)\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_sequence(sequence):\n",
    "#     \"\"\"Encodes a protein sequence using one-hot encoding.\"\"\"\n",
    "#     encoded_sequence = np.zeros((len(sequence), len(amino_acid_mapping)), dtype=int)\n",
    "#     for i, amino_acid in enumerate(sequence):\n",
    "#         index = amino_acid_mapping.get(amino_acid, amino_acid_mapping['X'])\n",
    "#         encoded_sequence[i, index] = 1\n",
    "#     return encoded_sequence\n",
    "\n",
    "# def normalize_pssm(pssm, normalize_method='min-max'):\n",
    "#     \"\"\"Normalizes a PSSM using the specified method.\"\"\"\n",
    "#     # Assuming the first two columns are non-numeric; adjust as necessary based on your actual data format\n",
    "#     numeric_columns = pssm[:, 2:]  # Adjust this if your numeric data starts from a different column\n",
    "\n",
    "#     # Convert to floats & handle any errors\n",
    "#     try:\n",
    "#         pssm_numeric = numeric_columns.astype(np.float32)\n",
    "#     except ValueError as e:\n",
    "#         raise ValueError(f\"Error converting PSSM to float: {e}\")\n",
    "\n",
    "#     if normalize_method == 'min-max':\n",
    "#         # Min-Max normalization\n",
    "#         pssm_min = pssm_numeric.min(axis=0)\n",
    "#         pssm_max = pssm_numeric.max(axis=0)\n",
    "#         # Ensure no division by zero\n",
    "#         pssm_range = np.where(pssm_max - pssm_min == 0, 1, pssm_max - pssm_min)\n",
    "#         normalized_pssm = (pssm_numeric - pssm_min) / pssm_range\n",
    "#     elif normalize_method == 'z-score':\n",
    "#         # Z-Score normalization\n",
    "#         pssm_mean = pssm_numeric.mean(axis=0)\n",
    "#         pssm_std = pssm_numeric.std(axis=0)\n",
    "#         # Avoid division by zero\n",
    "#         pssm_std = np.where(pssm_std == 0, 1, pssm_std)\n",
    "#         normalized_pssm = (pssm_numeric - pssm_mean) / pssm_std\n",
    "#     else:\n",
    "#         # If no normalization method provided, return the original PSSM\n",
    "#         normalized_pssm = pssm_numeric\n",
    "\n",
    "#     return normalized_pssm\n",
    "\n",
    "\n",
    "# def prepare_data_point(idx, seqs, protein_data, label_file=None):\n",
    "#     \"\"\"Prepares a protein sample for training or inference.\"\"\"\n",
    "#     labels = load_labels(label_file)\n",
    "#     protein_id = seqs.iloc[idx]['PDB_ID']\n",
    "#     sequence = seqs.iloc[idx]['SEQUENCE']\n",
    "#     encoded_sequence = encode_sequence(sequence)  # Encode the sequence\n",
    "#     pssm = protein_data[protein_id].values  # Assuming you will process PSSM separately\n",
    "#     normalized_pssm = normalize_pssm(pssm)  # Ensure this is uncommented to use normalized PSSM\n",
    "\n",
    "#     if labels is not None:\n",
    "#         label_seq = labels.iloc[idx]['SEC_STRUCT']\n",
    "#         label_numeric = [sec_struct_mapping[char] for char in label_seq]\n",
    "#         label_tensor = torch.tensor(label_numeric, dtype=torch.long)\n",
    "#         return (\n",
    "#             protein_id,\n",
    "#             torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "#             torch.tensor(normalized_pssm, dtype=torch.float32),\n",
    "#             label_tensor\n",
    "#         )\n",
    "\n",
    "#     return (\n",
    "#         protein_id,\n",
    "#         torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "#         torch.tensor(normalized_pssm, dtype=torch.float32)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_sequence(sequence, amino_acid_mapping):\n",
    "#     # Convert each amino acid in the sequence to a one-hot encoded vector\n",
    "#     encoded_sequence = np.zeros((len(sequence), len(amino_acid_mapping)), dtype=int)\n",
    "#     for i, amino_acid in enumerate(sequence):\n",
    "#         # Default to 'X' for unknown amino acids\n",
    "#         index = amino_acid_mapping.get(amino_acid, amino_acid_mapping['X'])\n",
    "#         encoded_sequence[i, index] = 1\n",
    "#     return encoded_sequence\n",
    "\n",
    "# def normalize_pssm(pssm, normalize_method='min-max'):\n",
    "#     # Assuming the first two columns are non-numeric; adjust as necessary based on your actual data format\n",
    "#     numeric_columns = pssm[:, 2:]  # Adjust this if your numeric data starts from a different column\n",
    "\n",
    "#     # Convert to floats\n",
    "#     try:\n",
    "#         pssm_numeric = numeric_columns.astype(np.float32)\n",
    "#     except ValueError as e:\n",
    "#         # Handle or log the error if needed\n",
    "#         raise ValueError(f\"Error converting PSSM to float: {e}\")\n",
    "\n",
    "#     if normalize_method == 'min-max':\n",
    "#         # Min-Max normalization\n",
    "#         pssm_min = pssm_numeric.min(axis=0)\n",
    "#         pssm_max = pssm_numeric.max(axis=0)\n",
    "#         # Ensure no division by zero\n",
    "#         pssm_range = np.where(pssm_max - pssm_min == 0, 1, pssm_max - pssm_min)\n",
    "#         normalized_pssm = (pssm_numeric - pssm_min) / pssm_range\n",
    "#     elif normalize_method == 'z-score':\n",
    "#         # Z-Score normalization\n",
    "#         pssm_mean = pssm_numeric.mean(axis=0)\n",
    "#         pssm_std = pssm_numeric.std(axis=0)\n",
    "#         # Avoid division by zero\n",
    "#         pssm_std = np.where(pssm_std == 0, 1, pssm_std)\n",
    "#         normalized_pssm = (pssm_numeric - pssm_mean) / pssm_std\n",
    "#     else:\n",
    "#         # If no normalization method provided, return the original PSSM\n",
    "#         normalized_pssm = pssm_numeric\n",
    "\n",
    "#     return normalized_pssm\n",
    "\n",
    "# def protein_dataset(csv_file, train_dir, label_file=None, normalize_method='min-max'):\n",
    "#     # Load the sequences\n",
    "#     seqs = pd.read_csv(csv_file)\n",
    "\n",
    "#     # Load the protein data from the directory\n",
    "#     protein_data = {}\n",
    "#     for filename in os.listdir(train_dir):\n",
    "#         if filename.endswith(\".csv\"):  # Check if the file is a CSV\n",
    "#             protein_id = re.split(r'_train|_test', filename)[0]\n",
    "#             protein_data[protein_id] = pd.read_csv(os.path.join(train_dir, filename))\n",
    "\n",
    "#     # Load the labels, if provided\n",
    "#     if label_file:\n",
    "#         labels = pd.read_csv(label_file)\n",
    "#     else:\n",
    "#         labels = None\n",
    "\n",
    "#     # Amino acid mapping\n",
    "#     amino_acid_mapping = {\n",
    "#         'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4,\n",
    "#         'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9,\n",
    "#         'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14,\n",
    "#         'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19,\n",
    "#         'X': 20,  # Typically used for unknown amino acids\n",
    "#         'B': 21,  # Asparagine or Aspartic acid\n",
    "#         'Z': 22,  # Glutamine or Glutamic acid\n",
    "#         'J': 23,  # Leucine or Isoleucine\n",
    "#         '-': 24,  # Gap or padding\n",
    "#     }\n",
    "\n",
    "# def get_item(idx):\n",
    "#     protein_id = seqs.iloc[idx]['PDB_ID']\n",
    "#     sequence = seqs.iloc[idx]['SEQUENCE']\n",
    "#     encoded_sequence = encode_sequence(sequence, amino_acid_mapping)  # Encode the sequence\n",
    "#     pssm = protein_data[protein_id].values  # Assuming you will process PSSM separately\n",
    "#     normalized_pssm = normalize_pssm(pssm, normalize_method)  # Ensure this is uncommented to use normalized PSSM\n",
    "\n",
    "#     if labels is not None:\n",
    "#         label_seq = labels.iloc[idx]['SEC_STRUCT']\n",
    "#         label_numeric = [sec_struct_mapping[char] for char in label_seq]\n",
    "#         label_tensor = torch.tensor(label_numeric, dtype=torch.long)\n",
    "#         return (\n",
    "#             protein_id,\n",
    "#             torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "#             torch.tensor(normalized_pssm, dtype=torch.float32),\n",
    "#             label_tensor\n",
    "#         )\n",
    "\n",
    "#     return (\n",
    "#         protein_id,\n",
    "#         torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "#         torch.tensor(normalized_pssm, dtype=torch.float32)\n",
    "#     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, csv_file, train_dir, label_file=None, normalize_method='min-max'):\n",
    "\n",
    "        # Load the sequences\n",
    "        self.seqs = pd.read_csv(csv_file)\n",
    "\n",
    "        # Load the protein data from the directory\n",
    "        self.protein_data = {}\n",
    "        for filename in os.listdir(train_dir):\n",
    "            if filename.endswith(\".csv\"):  # Check if the file is a CSV\n",
    "                protein_id = re.split(r'_train|_test', filename)[0]\n",
    "                self.protein_data[protein_id] = pd.read_csv(os.path.join(train_dir, filename))\n",
    "\n",
    "        # Load the labels, if provided\n",
    "        if label_file:\n",
    "            self.labels = pd.read_csv(label_file)\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "        # Amino acid mapping\n",
    "        self.amino_acid_mapping = amino_acid_mapping\n",
    "        self.normalize_method = normalize_method\n",
    "\n",
    "    def encode_sequence(self, sequence):\n",
    "        # Convert each amino acid in the sequence to a one-hot encoded vector\n",
    "        encoded_sequence = np.zeros((len(sequence), len(self.amino_acid_mapping)), dtype=int)\n",
    "        for i, amino_acid in enumerate(sequence):\n",
    "            # Default to 'X' for unknown amino acids\n",
    "            index = self.amino_acid_mapping.get(amino_acid, self.amino_acid_mapping['X'])\n",
    "            encoded_sequence[i, index] = 1\n",
    "        return encoded_sequence\n",
    "\n",
    "    def normalize_pssm(self, pssm):\n",
    "        # Assuming the first two columns are non-numeric; adjust as necessary based on your actual data format\n",
    "        numeric_columns = pssm[:, 2:]  # Adjust this if your numeric data starts from a different column\n",
    "\n",
    "        # Convert to floats\n",
    "        try:\n",
    "            pssm_numeric = numeric_columns.astype(np.float32)\n",
    "        except ValueError as e:\n",
    "            # Handle or log the error if needed\n",
    "            raise ValueError(f\"Error converting PSSM to float: {e}\")\n",
    "\n",
    "        if self.normalize_method == 'min-max':\n",
    "            # Min-Max normalization\n",
    "            pssm_min = pssm_numeric.min(axis=0)\n",
    "            pssm_max = pssm_numeric.max(axis=0)\n",
    "            # Ensure no division by zero\n",
    "            pssm_range = np.where(pssm_max - pssm_min == 0, 1, pssm_max - pssm_min)\n",
    "            normalized_pssm = (pssm_numeric - pssm_min) / pssm_range\n",
    "        elif self.normalize_method == 'z-score':\n",
    "            # Z-Score normalization\n",
    "            pssm_mean = pssm_numeric.mean(axis=0)\n",
    "            pssm_std = pssm_numeric.std(axis=0)\n",
    "            # Avoid division by zero\n",
    "            pssm_std = np.where(pssm_std == 0, 1, pssm_std)\n",
    "            normalized_pssm = (pssm_numeric - pssm_mean) / pssm_std\n",
    "        else:\n",
    "            # If no normalization method provided, return the original PSSM\n",
    "            normalized_pssm = pssm_numeric\n",
    "\n",
    "        return normalized_pssm\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        protein_id = self.seqs.iloc[idx]['PDB_ID']\n",
    "        sequence = self.seqs.iloc[idx]['SEQUENCE']\n",
    "        encoded_sequence = self.encode_sequence(sequence)  # Encode the sequence\n",
    "        pssm = self.protein_data[protein_id].values  # Assuming you will process PSSM separately\n",
    "        normalized_pssm = self.normalize_pssm(pssm)  # Ensure this is uncommented to use normalized PSSM\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label_seq = self.labels.iloc[idx]['SEC_STRUCT']\n",
    "            label_numeric = [sec_struct_mapping[char] for char in label_seq]\n",
    "            label_tensor = torch.tensor(label_numeric, dtype=torch.long)\n",
    "            return (\n",
    "                protein_id,\n",
    "                torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "                torch.tensor(normalized_pssm, dtype=torch.float32),\n",
    "                label_tensor\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            protein_id,\n",
    "            torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "            torch.tensor(normalized_pssm, dtype=torch.float32)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConvolutionalProteinModel(nn.Module):\n",
    "    def __init__(self, num_classes=3, input_channels=20):  # 20 for amino acid one-hot, adjust if using PSSM\n",
    "        super(FullyConvolutionalProteinModel, self).__init__()\n",
    "\n",
    "        # Define convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        # Final layer that maps to the number of classes\n",
    "        self.final_conv = nn.Conv1d(in_channels=256, out_channels=num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with activation functions\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Apply final convolutional layer - no activation, as CrossEntropyLoss includes it\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # No softmax here, as nn.CrossEntropyLoss applies it internally.\n",
    "        # Transpose the output to match [batch_size, sequence_length, num_classes]\n",
    "        # This makes it easier to calculate loss later\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinModelTrainer:\n",
    "    def __init__(self, model, criterion, optimizer, train_dataset, val_dataset=None, test_dataset=None, batch_size=64):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "        if val_dataset:\n",
    "            self.val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=self.collate_fn)\n",
    "        else:\n",
    "            self.val_loader = None\n",
    "\n",
    "        if test_dataset:\n",
    "            self.test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=self.collate_fn_without_labels)\n",
    "        else:\n",
    "            self.test_loader = None\n",
    "\n",
    "    def collate_fn_without_labels(self, batch):\n",
    "        id, sequences, pssms = zip(*batch)\n",
    "\n",
    "        sequences_padded = pad_sequence([seq.clone().detach() for seq in sequences], batch_first=True)\n",
    "        pssms_padded = pad_sequence([pssm.clone().detach() for pssm in pssms], batch_first=True)\n",
    "\n",
    "        return id, sequences_padded, pssms_padded\n",
    "\n",
    "    def collate_fn(self,batch):\n",
    "        _, sequences, pssms, labels_list = zip(*batch)  # Unzip the batch\n",
    "\n",
    "        # Pad sequences and PSSMs\n",
    "        sequences_padded = pad_sequence([seq.clone().detach() for seq in sequences], batch_first=True)\n",
    "\n",
    "        pssms_padded = pad_sequence([pssm.clone().detach() for pssm in pssms], batch_first=True)\n",
    "\n",
    "        # Handling labels correctly\n",
    "        if labels_list[0] is not None:  # Check if labels exist\n",
    "            labels_padded = pad_sequence([label.clone().detach() for label in labels_list], batch_first=True)\n",
    "\n",
    "        else:\n",
    "            labels_padded = None\n",
    "\n",
    "        return sequences_padded, pssms_padded, labels_padded\n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train_epoch()\n",
    "            if self.val_loader:\n",
    "                self.validate()\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "\n",
    "        for sequences, pssms, labels in self.train_loader:\n",
    "            inputs = pssms.permute(0, 2, 1)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs.transpose(1, 2), labels)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 2)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.numel()\n",
    "\n",
    "        epoch_loss = running_loss / len(self.train_dataset)\n",
    "        epoch_acc = correct_preds / total_preds\n",
    "        print(f'Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sequences, pssms, labels in self.val_loader:\n",
    "                inputs = pssms.permute(0, 2, 1)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs.transpose(1, 2), labels)\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                _, predicted = torch.max(outputs, 2)\n",
    "                correct_preds += (predicted == labels).sum().item()\n",
    "                total_preds += labels.numel()\n",
    "\n",
    "        val_loss = running_loss / len(self.val_dataset)\n",
    "        val_acc = correct_preds / total_preds\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "    def test(self, output_file='./submission.csv'):\n",
    "        if self.test_loader:\n",
    "            self.test_model(self.test_loader, output_file)\n",
    "        else:\n",
    "            self.test_model_direct(self.test_dataset, output_file)\n",
    "\n",
    "    def test_model(self, test_loader, output_file):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sequences, pssms, labels in test_loader:\n",
    "                inputs = pssms.permute(0, 2, 1)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs.transpose(1, 2), labels)\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                _, predicted = torch.max(outputs, 2)\n",
    "                correct_preds += (predicted == labels).sum().item()\n",
    "                total_preds += labels.numel()\n",
    "\n",
    "        test_loss = running_loss / len(test_loader.dataset)\n",
    "        test_acc = correct_preds / total_preds\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "    def test_model_direct(self, test_dataset, output_file):\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(test_dataset)):\n",
    "                pdb_id, _, pssm = test_dataset[i]\n",
    "\n",
    "                input_pssm = pssm.unsqueeze(0).permute(0, 2, 1)\n",
    "\n",
    "                outputs = self.model(input_pssm)\n",
    "                _, predicted = torch.max(outputs, 2)\n",
    "\n",
    "                seq_len = pssm.shape[0]\n",
    "                for j in range(seq_len):\n",
    "                    residue_id = f\"{pdb_id}_{j + 1}\"\n",
    "                    structure_label = ['H', 'E', 'C'][predicted[0, j].item()]\n",
    "                    predictions.append([residue_id, structure_label])\n",
    "\n",
    "        pd.DataFrame(predictions, columns=['ID', 'STRUCTURE']).to_csv(output_file, index=False)\n",
    "        print(f'Submission file saved to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_optimizer(optimizer_type, model, lr, weight_decay):\n",
    "#     # Choose the optimizer based on the parameterization\n",
    "#     if optimizer_type == \"adam\":\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#     elif optimizer_type == \"sgd\":\n",
    "#         optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#     elif optimizer_type == \"rmsprop\":\n",
    "#         optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#     else:\n",
    "#         raise ValueError(\"Unknown optimizer\")\n",
    "\n",
    "#     return optimizer\n",
    "\n",
    "\n",
    "# def train_with_params(\n",
    "#         lr=0.001,\n",
    "#         batch_size=4,\n",
    "#         hidden_layers=5,\n",
    "#         dropout_rate=0.233246,\n",
    "#         weight_decay=0.0,\n",
    "#         optimizer='rmsprop',\n",
    "#         normalization='min-max',\n",
    "#         num_epochs=10,\n",
    "#         output_file='submission.csv'\n",
    "# ):\n",
    "#     train_dataset = ProteinDataset(csv_file=seqs_train_path, train_dir=train_path, label_file=labels_train_path,\n",
    "#                                    normalize_method=normalization)\n",
    "#     train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "#     test_dataset = ProteinDataset(csv_file=seqs_test_path, train_dir=test_path, normalize_method=normalization)\n",
    "\n",
    "#     # Splitting train_dataset into train and validation sets (adjust sizes as needed)\n",
    "#     train_size = int(0.8 * len(train_dataset))\n",
    "#     val_size = len(train_dataset) - train_size\n",
    "#     train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "#     val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "#     model = FullyConvolutionalProteinModel(hidden_layers_number=hidden_layers, dropout_rate=dropout_rate)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = get_optimizer(optimizer, model, lr, weight_decay)\n",
    "\n",
    "#     train_model(model, criterion, optimizer, train_dataloader, num_epochs)\n",
    "#     validate_model(model, criterion, val_loader)\n",
    "#     test_model_direct(model, test_dataset, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load training data\n",
    "# seqs_train, protein_data_train = load_protein_data(seqs_train_path, train_path)\n",
    "# labels_train = load_labels(labels_train_path)\n",
    "\n",
    "# # Prepare training samples (assuming collate_fn handles batching and shuffling)\n",
    "# train_samples = []\n",
    "# for idx in range(len(seqs_train)):\n",
    "#     protein_id = seqs_train.iloc[idx]['PDB_ID']\n",
    "#     sequence = seqs_train.iloc[idx]['SEQUENCE']\n",
    "#     pssm = protein_data_train[protein_id].values\n",
    "#     sample = prepare_protein_sample(\n",
    "#         protein_id, sequence, pssm, labels_train and labels_train.iloc[idx],  # Include label if exists\n",
    "#         amino_acid_mapping, normalize_method, sec_struct_mapping\n",
    "#     )\n",
    "#     train_samples.append(sample)\n",
    "\n",
    "# # Load testing data\n",
    "# seqs_test, protein_data_test = load_protein_data(seqs_test_path, test_path)\n",
    "\n",
    "# # Prepare testing samples (assuming collate_fn handles batching)\n",
    "# test_samples = []\n",
    "# for idx in range(len(seqs_test)):\n",
    "#     protein_id = seqs_test.iloc[idx]['PDB_ID']\n",
    "#     sequence = seqs_test.iloc[idx]['SEQUENCE']\n",
    "#     pssm = protein_data_test[protein_id].values\n",
    "#     sample = prepare_protein_sample(\n",
    "#         protein_id, sequence, pssm, labels_train.empty and None or labels_train.iloc[idx],  # Include label if exists\n",
    "#         amino_acid_mapping, normalize_method, sec_struct_mapping\n",
    "#     )\n",
    "\n",
    "#     test_samples.append(sample)\n",
    "\n",
    "# # Create dataloaders (assuming collate_fn remains the same)\n",
    "# train_dataloader = DataLoader(train_samples, batch_size=4, collate_fn=collate_fn)\n",
    "# test_dataloader = DataLoader(test_samples, batch_size=4, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seqs, protein_data = load_protein_data(seqs_train_path, train_path)\n",
    "# train_dataset = [prepare_data_point(idx, seqs, protein_data, labels_train_path) for idx in range(len(seqs))]\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "# test_dataset = [prepare_data_point(idx, seqs, protein_data, label_file=None) for idx in range(len(seqs))]\n",
    "\n",
    "# # Model definition and training...\n",
    "# model = FullyConvolutionalProteinModel()\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0)\n",
    "# num_epochs = 25\n",
    "\n",
    "# # Train and Test model on test dataset and create submission file\n",
    "# train_model(model, criterion, optimizer, train_dataloader, num_epochs)\n",
    "# test_model_direct(model, test_dataset, output_file='./data/submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2448, Train Accuracy: 0.8963\n",
      "Train Loss: 0.2041, Train Accuracy: 0.9183\n",
      "Train Loss: 0.1914, Train Accuracy: 0.9238\n",
      "Train Loss: 0.1850, Train Accuracy: 0.9268\n",
      "Train Loss: 0.1780, Train Accuracy: 0.9301\n",
      "Train Loss: 0.1713, Train Accuracy: 0.9323\n",
      "Train Loss: 0.1718, Train Accuracy: 0.9319\n",
      "Train Loss: 0.1703, Train Accuracy: 0.9332\n",
      "Train Loss: 0.1695, Train Accuracy: 0.9336\n",
      "Train Loss: 0.1680, Train Accuracy: 0.9345\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 20, 3], expected input[64, 25, 696] to have 20 channels, but got 25 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain(num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 105\u001b[0m, in \u001b[0;36mProteinModelTrainer.test\u001b[0;34m(self, output_file)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./submission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loader:\n\u001b[0;32m--> 105\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loader, output_file)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_model_direct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_dataset, output_file)\n",
      "Cell \u001b[0;32mIn[8], line 119\u001b[0m, in \u001b[0;36mProteinModelTrainer.test_model\u001b[0;34m(self, test_loader, output_file)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sequences, pssms, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m    117\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m pssms\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs)\n\u001b[1;32m    120\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), labels)\n\u001b[1;32m    122\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m, in \u001b[0;36mFullyConvolutionalProteinModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Apply convolutional layers with activation functions\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[1;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    307\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 20, 3], expected input[64, 25, 696] to have 20 channels, but got 25 channels instead"
     ]
    }
   ],
   "source": [
    "# Initialize datasets\n",
    "train_dataset = ProteinDataset(csv_file=seqs_train_path, train_dir=train_path, label_file=labels_train_path)\n",
    "test_dataset = ProteinDataset(csv_file=seqs_test_path, train_dir=test_path)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model = FullyConvolutionalProteinModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0)\n",
    "\n",
    "# Create an instance of the ProteinModelTrainer\n",
    "trainer = ProteinModelTrainer(model, criterion, optimizer, train_dataset, test_dataset=test_dataset, batch_size=64)\n",
    "\n",
    "# Train the model\n",
    "trainer.train(num_epochs=10)\n",
    "\n",
    "# Test the model\n",
    "trainer.test(output_file='submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
