{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [307, 25] at entry 0 and [142, 25] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 244\u001b[0m\n\u001b[1;32m    241\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mRMSprop(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m    242\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m--> 244\u001b[0m train_model(model, criterion, optimizer, train_dataloader)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Test model on test dataset and create submission file\u001b[39;00m\n\u001b[1;32m    247\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m create_dataloader(seqs_test_path, test_path)\n",
      "Cell \u001b[0;32mIn[5], line 185\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, train_dataloader, num_epochs)\u001b[0m\n\u001b[1;32m    182\u001b[0m correct_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    183\u001b[0m total_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sequences, pssms, labels, _ \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m    186\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m pssms\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Adjust for PSSM data\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [307, 25] at entry 0 and [142, 25] at entry 1"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Define store file paths\n",
    "DATA_PATH = \"./data/\"\n",
    "labels_train_path = DATA_PATH + \"labels_train.csv\"\n",
    "sample_path = DATA_PATH + \"sample.csv\"\n",
    "seqs_test_path = DATA_PATH + \"seqs_test.csv\"\n",
    "seqs_train_path = DATA_PATH + \"seqs_train.csv\"\n",
    "train_path = DATA_PATH + \"train\"\n",
    "test_path = DATA_PATH + \"test\"\n",
    "\n",
    "# Define a mapping from amino acid characters to integers\n",
    "amino_acid_mapping = {\n",
    "    'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4,\n",
    "    'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9,\n",
    "    'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14,\n",
    "    'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19,\n",
    "    'X': 20,  # Typically used for unknown amino acids\n",
    "    'B': 21,  # Asparagine or Aspartic acid\n",
    "    'Z': 22,  # Glutamine or Glutamic acid\n",
    "    'J': 23,  # Leucine or Isoleucine\n",
    "    '-': 24,  # Gap or padding\n",
    "}\n",
    "\n",
    "sec_struct_mapping = {'H': 0, 'E': 1, 'C': 2}  # Add more mappings if there are more labels\n",
    "\n",
    "\n",
    "def load_data(csv_file, train_dir):\n",
    "    # Load the sequences\n",
    "    seqs = pd.read_csv(csv_file)\n",
    "\n",
    "    # Load the protein data from the directory\n",
    "    protein_data = {}\n",
    "    for filename in os.listdir(train_dir):\n",
    "        if filename.endswith(\".csv\"):  # Check if the file is a CSV\n",
    "            protein_id = re.split(r'_train|_test', filename)[0]\n",
    "            protein_data[protein_id] = pd.read_csv(os.path.join(train_dir, filename))\n",
    "\n",
    "    return seqs, protein_data\n",
    "\n",
    "\n",
    "def encode_sequence(sequence):\n",
    "    # Convert each amino acid in the sequence to a one-hot encoded vector\n",
    "    encoded_sequence = np.zeros((len(sequence), len(amino_acid_mapping)), dtype=int)\n",
    "    for i, amino_acid in enumerate(sequence):\n",
    "        # Default to 'X' for unknown amino acids\n",
    "        index = amino_acid_mapping.get(amino_acid, amino_acid_mapping['X'])\n",
    "        encoded_sequence[i, index] = 1\n",
    "    return encoded_sequence\n",
    "\n",
    "\n",
    "def normalize_pssm(pssm, normalize_method='min-max'):\n",
    "    # Assuming the first two columns are non-numeric; adjust as necessary based on your actual data format\n",
    "    numeric_columns = pssm[:, 2:]  # Adjust this if your numeric data starts from a different column\n",
    "\n",
    "    # Convert to floats\n",
    "    try:\n",
    "        pssm_numeric = numeric_columns.astype(np.float32)\n",
    "    except ValueError as e:\n",
    "        # Handle or log the error if needed\n",
    "        raise ValueError(f\"Error converting PSSM to float: {e}\")\n",
    "\n",
    "    if normalize_method == 'min-max':\n",
    "        # Min-Max normalization\n",
    "        pssm_min = pssm_numeric.min(axis=0)\n",
    "        pssm_max = pssm_numeric.max(axis=0)\n",
    "        # Ensure no division by zero\n",
    "        pssm_range = np.where(pssm_max - pssm_min == 0, 1, pssm_max - pssm_min)\n",
    "        normalized_pssm = (pssm_numeric - pssm_min) / pssm_range\n",
    "    elif normalize_method == 'z-score':\n",
    "        # Z-Score normalization\n",
    "        pssm_mean = pssm_numeric.mean(axis=0)\n",
    "        pssm_std = pssm_numeric.std(axis=0)\n",
    "        # Avoid division by zero\n",
    "        pssm_std = np.where(pssm_std == 0, 1, pssm_std)\n",
    "        normalized_pssm = (pssm_numeric - pssm_mean) / pssm_std\n",
    "    else:\n",
    "        # If no normalization method provided, return the original PSSM\n",
    "        normalized_pssm = pssm_numeric\n",
    "\n",
    "    return normalized_pssm\n",
    "\n",
    "\n",
    "def prepare_data_point(idx, seqs, protein_data, label_file=None):\n",
    "    # Load the labels, if provided\n",
    "    if label_file:\n",
    "        labels = pd.read_csv(label_file)\n",
    "    else:\n",
    "        labels = None\n",
    "\n",
    "    protein_id = seqs.iloc[idx]['PDB_ID']\n",
    "    sequence = seqs.iloc[idx]['SEQUENCE']\n",
    "    encoded_sequence = encode_sequence(sequence)  # Encode the sequence\n",
    "    pssm = protein_data[protein_id].values  # Assuming you will process PSSM separately\n",
    "    normalized_pssm = normalize_pssm(pssm)  # Ensure this is uncommented to use normalized PSSM\n",
    "\n",
    "    if labels is not None:\n",
    "        label_seq = labels.iloc[idx]['SEC_STRUCT']\n",
    "        label_numeric = [sec_struct_mapping[char] for char in label_seq]\n",
    "        label_tensor = torch.tensor(label_numeric, dtype=torch.long)\n",
    "        return (\n",
    "            protein_id,\n",
    "            torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "            torch.tensor(normalized_pssm, dtype=torch.float32),\n",
    "            label_tensor\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        protein_id,\n",
    "        torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "        torch.tensor(normalized_pssm, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "\n",
    "def create_dataloader(csv_file, train_dir, label_file=None, batch_size=4):\n",
    "    seqs, protein_data = load_data(csv_file, train_dir)\n",
    "    data = [prepare_data_point(idx, seqs, protein_data, label_file) for idx in range(len(seqs))]\n",
    "    return DataLoader(data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    _, sequences, pssms, labels_list = zip(*batch)  # Unzip the batch\n",
    "\n",
    "    # Pad sequences and PSSMs\n",
    "    sequences_padded = pad_sequence([seq.clone().detach() for seq in sequences], batch_first=True)\n",
    "\n",
    "    pssms_padded = pad_sequence([pssm.clone().detach() for pssm in pssms], batch_first=True)\n",
    "\n",
    "    # Handling labels correctly\n",
    "    if labels_list[0] is not None:  # Check if labels exist\n",
    "        labels_padded = pad_sequence([label.clone().detach() for label in labels_list], batch_first=True)\n",
    "\n",
    "    else:\n",
    "        labels_padded = None\n",
    "\n",
    "    # Create a mask based on the original sequence lengths\n",
    "    mask = [torch.ones(len(label), dtype=torch.uint8) for label in labels_list]\n",
    "    mask_padded = pad_sequence(mask, batch_first=True, padding_value=0)  # Assuming padding_value for labels is 0\n",
    "    return sequences_padded, pssms_padded, labels_padded, mask_padded\n",
    "\n",
    "\n",
    "class FullyConvolutionalProteinModel(torch.nn.Module):\n",
    "    def __init__(self, num_classes=3, input_channels=20):  # 20 for amino acid one-hot, adjust if using PSSM\n",
    "        super(FullyConvolutionalProteinModel, self).__init__()\n",
    "\n",
    "        # Define convolutional layers\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3 = torch.nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        # Final layer that maps to the number of classes\n",
    "        self.final_conv = torch.nn.Conv1d(in_channels=256, out_channels=num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with activation functions\n",
    "        x = torch.nn.functional.relu(self.conv1(x))\n",
    "        x = torch.nn.functional.relu(self.conv2(x))\n",
    "        x = torch.nn.functional.relu(self.conv3(x))\n",
    "\n",
    "        # Apply final convolutional layer - no activation, as CrossEntropyLoss includes it\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # No softmax here, as nn.CrossEntropyLoss applies it internally.\n",
    "        # Transpose the output to match [batch_size, sequence_length, num_classes]\n",
    "        # This makes it easier to calculate loss later\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_dataloader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "\n",
    "        for sequences, pssms, labels, _ in train_dataloader:\n",
    "            inputs = pssms.permute(0, 2, 1)  # Adjust for PSSM data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.transpose(1, 2), labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs, 2)  # Get the index of the max log-probability\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.numel()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "        epoch_acc = correct_preds / total_preds\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "\n",
    "def test_model_direct(model, test_dataset, output_file='./submission.csv'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_dataset)):  # Iterate directly over the dataset\n",
    "            pdb_id, _, pssm = test_dataset[i]  # Assuming the dataset returns PDB_ID, sequence, and PSSM\n",
    "\n",
    "            # Prepare the input tensor; add an extra batch dimension using unsqueeze\n",
    "            input_pssm = pssm.unsqueeze(0).permute(0, 2, 1)  # Adjust dimensions to [1, features, seq_len]\n",
    "\n",
    "            # Make a prediction\n",
    "            outputs = model(input_pssm)\n",
    "            _, predicted = torch.max(outputs, 2)  # Get the index of max log-probability\n",
    "\n",
    "            # Process the predictions\n",
    "            seq_len = pssm.shape[0]  # Assuming pssm is [features, seq_len]\n",
    "            for j in range(seq_len):\n",
    "                residue_id = f\"{pdb_id}_{j + 1}\"  # Construct the ID\n",
    "                structure_label = ['H', 'E', 'C'][predicted[0, j].item()]  # Map numeric predictions to labels\n",
    "                predictions.append([residue_id, structure_label])\n",
    "\n",
    "    # Write predictions to CSV\n",
    "    pd.DataFrame(predictions, columns=['ID', 'STRUCTURE']).to_csv(output_file, index=False)\n",
    "    print(f'Submission file saved to {output_file}')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dataloader = create_dataloader(seqs_train_path, train_path, label_file=labels_train_path, batch_size=4)\n",
    "\n",
    "    # Model definition and training...\n",
    "    model = FullyConvolutionalProteinModel()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0)\n",
    "    num_epochs = 10\n",
    "\n",
    "    train_model(model, criterion, optimizer, train_dataloader)\n",
    "\n",
    "    # Test model on test dataset and create submission file\n",
    "    test_dataset = create_dataloader(seqs_test_path, test_path)\n",
    "    test_model_direct(model, test_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
