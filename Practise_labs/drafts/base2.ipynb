{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/25, Loss: 0.2478, Accuracy: 0.9011\n",
      "Epoch 2/25, Loss: 0.2023, Accuracy: 0.9192\n",
      "Epoch 3/25, Loss: 0.1982, Accuracy: 0.9215\n",
      "Epoch 4/25, Loss: 0.1857, Accuracy: 0.9263\n",
      "Epoch 5/25, Loss: 0.1801, Accuracy: 0.9284\n",
      "Epoch 6/25, Loss: 0.1736, Accuracy: 0.9309\n",
      "Epoch 7/25, Loss: 0.1766, Accuracy: 0.9306\n",
      "Epoch 8/25, Loss: 0.1759, Accuracy: 0.9310\n",
      "Epoch 9/25, Loss: 0.1739, Accuracy: 0.9313\n",
      "Epoch 10/25, Loss: 0.1768, Accuracy: 0.9310\n",
      "Epoch 11/25, Loss: 0.1724, Accuracy: 0.9324\n",
      "Epoch 12/25, Loss: 0.1717, Accuracy: 0.9324\n",
      "Epoch 13/25, Loss: 0.1693, Accuracy: 0.9335\n",
      "Epoch 14/25, Loss: 0.1654, Accuracy: 0.9350\n",
      "Epoch 15/25, Loss: 0.1662, Accuracy: 0.9349\n",
      "Epoch 16/25, Loss: 0.1667, Accuracy: 0.9347\n",
      "Epoch 17/25, Loss: 0.1642, Accuracy: 0.9356\n",
      "Epoch 18/25, Loss: 0.1593, Accuracy: 0.9371\n",
      "Epoch 19/25, Loss: 0.1604, Accuracy: 0.9365\n",
      "Epoch 20/25, Loss: 0.1641, Accuracy: 0.9359\n",
      "Epoch 21/25, Loss: 0.1615, Accuracy: 0.9368\n",
      "Epoch 22/25, Loss: 0.1612, Accuracy: 0.9372\n",
      "Epoch 23/25, Loss: 0.1581, Accuracy: 0.9378\n",
      "Epoch 24/25, Loss: 0.1580, Accuracy: 0.9381\n",
      "Epoch 25/25, Loss: 0.1601, Accuracy: 0.9373\n",
      "Validation Loss: 0.1735, Validation Accuracy: 0.9335\n",
      "Submission file saved to ./submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Define store file paths\n",
    "DATA_PATH = \"./data/\"\n",
    "labels_train_path = DATA_PATH + \"labels_train.csv\"\n",
    "sample_path = DATA_PATH + \"sample.csv\"\n",
    "seqs_test_path = DATA_PATH + \"seqs_test.csv\"\n",
    "seqs_train_path = DATA_PATH + \"seqs_train.csv\"\n",
    "train_path = DATA_PATH + \"train\"\n",
    "test_path = DATA_PATH + \"test\"\n",
    "\n",
    "# Define a mapping from amino acid characters to integers\n",
    "amino_acid_mapping = {\n",
    "    'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4,\n",
    "    'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9,\n",
    "    'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14,\n",
    "    'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19,\n",
    "    'X': 20,  # Typically used for unknown amino acids\n",
    "    'B': 21,  # Asparagine or Aspartic acid\n",
    "    'Z': 22,  # Glutamine or Glutamic acid\n",
    "    'J': 23,  # Leucine or Isoleucine\n",
    "    '-': 24,  # Gap or padding\n",
    "}\n",
    "\n",
    "sec_struct_mapping = {'H': 0, 'E': 1, 'C': 2}  # Add more mappings if there are more labels\n",
    "\n",
    "# Check if a CUDA-enabled GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, csv_file, train_dir, label_file=None, normalize_method='min-max'):\n",
    "\n",
    "        # Load the sequences\n",
    "        self.seqs = pd.read_csv(csv_file)\n",
    "\n",
    "        # Load the protein data from the directory\n",
    "        self.protein_data = {}\n",
    "        for filename in os.listdir(train_dir):\n",
    "            if filename.endswith(\".csv\"):  # Check if the file is a CSV\n",
    "                protein_id = re.split(r'_train|_test', filename)[0]\n",
    "                self.protein_data[protein_id] = pd.read_csv(os.path.join(train_dir, filename))\n",
    "\n",
    "        # Load the labels, if provided\n",
    "        if label_file:\n",
    "            self.labels = pd.read_csv(label_file)\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "        # Amino acid mapping\n",
    "        self.amino_acid_mapping = amino_acid_mapping\n",
    "        self.normalize_method = normalize_method\n",
    "\n",
    "    def encode_sequence(self, sequence):\n",
    "        # Convert each amino acid in the sequence to a one-hot encoded vector\n",
    "        encoded_sequence = np.zeros((len(sequence), len(self.amino_acid_mapping)), dtype=int)\n",
    "        for i, amino_acid in enumerate(sequence):\n",
    "            # Default to 'X' for unknown amino acids\n",
    "            index = self.amino_acid_mapping.get(amino_acid, self.amino_acid_mapping['X'])\n",
    "            encoded_sequence[i, index] = 1\n",
    "        return encoded_sequence\n",
    "\n",
    "    def normalize_pssm(self, pssm):\n",
    "        # Assuming the first two columns are non-numeric; adjust as necessary based on your actual data format\n",
    "        numeric_columns = pssm[:, 2:]  # Adjust this if your numeric data starts from a different column\n",
    "\n",
    "        # Convert to floats\n",
    "        try:\n",
    "            pssm_numeric = numeric_columns.astype(np.float32)\n",
    "        except ValueError as e:\n",
    "            # Handle or log the error if needed\n",
    "            raise ValueError(f\"Error converting PSSM to float: {e}\")\n",
    "\n",
    "        if self.normalize_method == 'min-max':\n",
    "            # Min-Max normalization\n",
    "            pssm_min = pssm_numeric.min(axis=0)\n",
    "            pssm_max = pssm_numeric.max(axis=0)\n",
    "            # Ensure no division by zero\n",
    "            pssm_range = np.where(pssm_max - pssm_min == 0, 1, pssm_max - pssm_min)\n",
    "            normalized_pssm = (pssm_numeric - pssm_min) / pssm_range\n",
    "        elif self.normalize_method == 'z-score':\n",
    "            # Z-Score normalization\n",
    "            pssm_mean = pssm_numeric.mean(axis=0)\n",
    "            pssm_std = pssm_numeric.std(axis=0)\n",
    "            # Avoid division by zero\n",
    "            pssm_std = np.where(pssm_std == 0, 1, pssm_std)\n",
    "            normalized_pssm = (pssm_numeric - pssm_mean) / pssm_std\n",
    "        else:\n",
    "            # If no normalization method provided, return the original PSSM\n",
    "            normalized_pssm = pssm_numeric\n",
    "\n",
    "        return normalized_pssm\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        protein_id = self.seqs.iloc[idx]['PDB_ID']\n",
    "        sequence = self.seqs.iloc[idx]['SEQUENCE']\n",
    "        encoded_sequence = self.encode_sequence(sequence)  # Encode the sequence\n",
    "        pssm = self.protein_data[protein_id].values  # Assuming you will process PSSM separately\n",
    "        normalized_pssm = self.normalize_pssm(pssm)  # Ensure this is uncommented to use normalized PSSM\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label_seq = self.labels.iloc[idx]['SEC_STRUCT']\n",
    "            label_numeric = [sec_struct_mapping[char] for char in label_seq]\n",
    "            label_tensor = torch.tensor(label_numeric, dtype=torch.long)\n",
    "            return (\n",
    "                protein_id,\n",
    "                torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "                torch.tensor(normalized_pssm, dtype=torch.float32),\n",
    "                label_tensor\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            protein_id,\n",
    "            torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "            torch.tensor(normalized_pssm, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "\n",
    "def collate_fn_without_labels(batch):\n",
    "    id, sequences, pssms = zip(*batch)  # Unzip the batch\n",
    "\n",
    "    # Pad sequences and PSSMs\n",
    "    sequences_padded = pad_sequence([seq.clone().detach() for seq in sequences], batch_first=True)\n",
    "    # sequences_padded =  torch.tensor([seq.clone().detach() for seq in sequences])\n",
    "    pssms_padded = pad_sequence([pssm.clone().detach() for pssm in pssms], batch_first=True)\n",
    "\n",
    "#     pssms_padded = torch.tensor(pssms)\n",
    "    # pssms_padded = pad_sequence([pssm.clone().detach() for pssm in pssms], batch_first=True)\n",
    "\n",
    "    return id, sequences_padded, pssms_padded\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    _, sequences, pssms, labels_list = zip(*batch)  # Unzip the batch\n",
    "\n",
    "    # Pad sequences and PSSMs\n",
    "    sequences_padded = pad_sequence([seq.clone().detach() for seq in sequences], batch_first=True)\n",
    "\n",
    "    pssms_padded = pad_sequence([pssm.clone().detach() for pssm in pssms], batch_first=True)\n",
    "\n",
    "    # Handling labels correctly\n",
    "    if labels_list[0] is not None:  # Check if labels exist\n",
    "        labels_padded = pad_sequence([label.clone().detach() for label in labels_list], batch_first=True)\n",
    "\n",
    "    else:\n",
    "        labels_padded = None\n",
    "\n",
    "    # Create a mask based on the original sequence lengths\n",
    "    mask = [torch.ones(len(label), dtype=torch.uint8) for label in labels_list]\n",
    "    mask_padded = pad_sequence(mask, batch_first=True, padding_value=0)  # Assuming padding_value for labels is 0\n",
    "    return sequences_padded, pssms_padded, labels_padded, mask_padded\n",
    "\n",
    "\n",
    "class FullyConvolutionalProteinModel(nn.Module):\n",
    "    def __init__(self, num_classes=3, input_channels=20):  # 20 for amino acid one-hot, adjust if using PSSM\n",
    "        super(FullyConvolutionalProteinModel, self).__init__()\n",
    "\n",
    "        # Define convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        # Final layer that maps to the number of classes\n",
    "        self.final_conv = nn.Conv1d(in_channels=256, out_channels=num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with activation functions\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Apply final convolutional layer - no activation, as CrossEntropyLoss includes it\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # No softmax here, as nn.CrossEntropyLoss applies it internally.\n",
    "        # Transpose the output to match [batch_size, sequence_length, num_classes]\n",
    "        # This makes it easier to calculate loss later\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ProteinModelTrainer:\n",
    "    def __init__(self, model, criterion, optimizer, train_dataset, val_dataset, test_dataset, batch_size=64):\n",
    "        self.model = model.to(device)  # Move the model to the device\n",
    "        self.criterion = criterion.to(device)  # Move the loss function to the device\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        self.val_loader = DataLoader(self.val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_without_labels)\n",
    "\n",
    "    def train_model(self, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()  # Set model to training mode\n",
    "            running_loss = 0.0\n",
    "            correct_preds = 0\n",
    "            total_preds = 0\n",
    "\n",
    "            for sequences, pssms, labels, _ in self.train_loader:\n",
    "                inputs = pssms.permute(0, 2, 1).to(device)  # Move input to device\n",
    "                labels = labels.to(device)  # Move labels to device\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs.transpose(1, 2), labels)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate training accuracy\n",
    "                _, predicted = torch.max(outputs, 2)  # Get the index of the max log-probability\n",
    "                correct_preds += (predicted == labels).sum().item()\n",
    "                total_preds += labels.numel()\n",
    "\n",
    "            epoch_loss = running_loss / len(self.train_loader.dataset)\n",
    "            epoch_acc = correct_preds / total_preds\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "    def validate_model(self):\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sequences, pssms, labels, _ in self.val_loader:\n",
    "                inputs = pssms.permute(0, 2, 1).to(device)  # Move input to device\n",
    "                labels = labels.to(device)  # Move labels to device\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs.transpose(1, 2), labels)\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 2)\n",
    "                correct_preds += (predicted == labels).sum().item()\n",
    "                total_preds += labels.numel()\n",
    "\n",
    "        val_loss = running_loss / len(self.val_loader.dataset)\n",
    "        val_acc = correct_preds / total_preds\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "    def test_model(self):\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sequences, pssms, labels, *_ in self.test_loader:\n",
    "                inputs = pssms.permute(0, 2, 1).to(device)  # Move input to device\n",
    "                labels = labels.to(device)  # Move labels to device\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs.transpose(1, 2), labels)\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 2)\n",
    "                correct_preds += (predicted == labels).sum().item()\n",
    "                total_preds += labels.numel()\n",
    "\n",
    "        test_loss = running_loss / len(self.test_loader.dataset)\n",
    "        test_acc = correct_preds / total_preds\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "    def test_model_direct(self, output_file='./submission.csv'):\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(self.test_dataset)):  # Iterate directly over the dataset\n",
    "                pdb_id, _, pssm = self.test_dataset[i]  # Assuming the dataset returns PDB_ID, sequence, and PSSM\n",
    "\n",
    "                # Prepare the input tensor; add an extra batch dimension using unsqueeze\n",
    "                input_pssm = pssm.unsqueeze(0).permute(0, 2, 1).to(device)  # Move input to device\n",
    "\n",
    "                # Make a prediction\n",
    "                outputs = self.model(input_pssm)\n",
    "                _, predicted = torch.max(outputs, 2)  # Get the index of max log-probability\n",
    "\n",
    "                # Process the predictions\n",
    "                seq_len = pssm.shape[0]  # Assuming pssm is [features, seq_len]\n",
    "                for j in range(seq_len):\n",
    "                    residue_id = f\"{pdb_id}_{j + 1}\"  # Construct the ID\n",
    "                    structure_label = ['H', 'E', 'C'][predicted[0, j].item()]  # Map numeric predictions to labels\n",
    "                    predictions.append([residue_id, structure_label])\n",
    "\n",
    "        # Write predictions to CSV\n",
    "        pd.DataFrame(predictions, columns=['ID', 'STRUCTURE']).to_csv(output_file, index=False)\n",
    "        print(f'Submission file saved to {output_file}')\n",
    "\n",
    "# Create datasets\n",
    "dataset = ProteinDataset(csv_file=seqs_train_path, train_dir=train_path, label_file=labels_train_path)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_subset, val_subset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "test_dataset = ProteinDataset(csv_file=seqs_test_path, train_dir=test_path)\n",
    "\n",
    "# Create model, loss function, and optimizer\n",
    "model = FullyConvolutionalProteinModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0)\n",
    "\n",
    "# Create trainer instance\n",
    "trainer = ProteinModelTrainer(model, criterion, optimizer, train_subset, val_subset, test_dataset, batch_size=64)\n",
    "\n",
    "# Train and evaluate the model\n",
    "num_epochs = 25\n",
    "trainer.train_model(num_epochs)\n",
    "trainer.validate_model()\n",
    "# trainer.test_model()\n",
    "trainer.test_model_direct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from ax.service.ax_client import AxClient\n",
    "from ax.service.utils.instantiation import ObjectiveProperties\n",
    "from ax.utils.tutorials.cnn_utils import evaluate, load_mnist, train\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_validate_cv(model, x_train, y_train, params, n_folds=5):\n",
    "  \"\"\"\n",
    "  Performs K-Fold cross-validation to evaluate model performance.\n",
    "\n",
    "  Args:\n",
    "      model: ProteinModelTrainer object representing the protein structure prediction model.\n",
    "      x_train: Training data (protein sequences).\n",
    "      y_train: Training labels (protein structures).\n",
    "      params: Dictionary containing hyperparameter values for the model.\n",
    "      n_folds: Number of folds for cross-validation (default: 5).\n",
    "\n",
    "  Returns:\n",
    "      Average precision score across all folds.\n",
    "  \"\"\"\n",
    "\n",
    "  # Define KFold object\n",
    "  kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "  # Initialize variables for tracking performance\n",
    "  total_precision = 0\n",
    "\n",
    "  # Perform cross-validation\n",
    "  for train_index, val_index in kf.split(x_train):\n",
    "    x_train_fold, x_val_fold = x_train[train_index], x_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    # Train model on the current fold with specified hyperparameters\n",
    "    model.train(x_train_fold, y_train_fold, params=params)\n",
    "\n",
    "    # Evaluate model precision on the validation fold\n",
    "    precision = model.evaluate(x_val_fold, y_val_fold)[\"precision\"]\n",
    "\n",
    "    # Accumulate precision across folds\n",
    "    total_precision += precision\n",
    "\n",
    "  # Calculate average precision\n",
    "  avg_precision = total_precision / n_folds\n",
    "\n",
    "  return avg_precision\n",
    "\n",
    "\n",
    "def train_evaluate(parameterization):\n",
    "    avg_precision = train_validate_cv(\n",
    "        num_folds=3,\n",
    "        input_type=\"pssms\",  # \"sequence\" or \"pssms\"\n",
    "        lr=parameterization[\"lr\"],\n",
    "        batch_size=parameterization[\"batch_size\"],\n",
    "        hidden_layers=parameterization.get(\"hidden_layers\", 3),\n",
    "        dropout_rate=parameterization.get(\"dropout_rate\", 0.5),\n",
    "        weight_decay=parameterization.get(\"weight_decay\", 0),\n",
    "        optimizer_type=parameterization[\"optimizer\"],\n",
    "        normalization=parameterization[\"normalization\"],\n",
    "        num_epochs=parameterization.get(\"epochs\", 10),\n",
    "    )\n",
    "    return {\"precision\": avg_precision}\n",
    "\n",
    "# Set up the Ax experiment\n",
    "ax_client = AxClient(enforce_sequential_optimization=False)\n",
    "ax_client.create_experiment(\n",
    "    name=\"protein_model_experiment\",\n",
    "    parameters=[\n",
    "        {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [0.0001, 0.01], \"log_scale\": True},\n",
    "        {\"name\": \"batch_size\", \"type\": \"fixed\", \"value\": 4},\n",
    "        {\"name\": \"hidden_layers\", \"type\": \"choice\", \"values\": [4, 5]},\n",
    "        {\"name\": \"dropout_rate\", \"type\": \"range\", \"bounds\": [0.0, 0.7]},\n",
    "        {\"name\": \"weight_decay\", \"type\": \"range\", \"bounds\": [0.0, 0.1]},\n",
    "        {\"name\": \"epochs\", \"type\": \"fixed\", \"value\": 10},  # Fixed for all trials, can be changed as needed\n",
    "        {\"name\": \"optimizer\", \"type\": \"fixed\", \"value\": \"rmsprop\"},  # Add optimizer as a choice\n",
    "        {\"name\": \"normalization\", \"type\": \"fixed\", \"value\": \"min-max\"},\n",
    "    ],\n",
    "    objectives={\"precision\": ObjectiveProperties(minimize=False)}\n",
    ")\n",
    "\n",
    "# Running the trials\n",
    "for i in range(20):  # Number of iterations\n",
    "    params, trial_index = ax_client.get_next_trial()\n",
    "    metrics = train_evaluate(params)\n",
    "    ax_client.complete_trial(trial_index=trial_index, raw_data=metrics)\n",
    "\n",
    "# Fetch the best parameters\n",
    "best_parameters, metrics = ax_client.get_best_parameters()\n",
    "print(f'Best Parameters: {best_parameters}')\n",
    "print(metrics)\n",
    "best_metrics = metrics['objectives']\n",
    "print(f'Best Precision: {best_metrics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seqs, protein_data = load_protein_data(seqs_train_path, train_path)\n",
    "# train_dataset = [prepare_data_point(idx, seqs, protein_data, labels_train_path) for idx in range(len(seqs))]\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "# test_dataset = [prepare_data_point(idx, seqs, protein_data, label_file=None) for idx in range(len(seqs))]\n",
    "\n",
    "# # Model definition and training...\n",
    "# model = FullyConvolutionalProteinModel()\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0)\n",
    "# num_epochs = 25\n",
    "\n",
    "# # Train and Test model on test dataset and create submission file\n",
    "# train_model(model, criterion, optimizer, train_dataloader, num_epochs)\n",
    "# test_model_direct(model, test_dataset, output_file='./data/submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_protein_data(csv_file, train_dir):\n",
    "#     \"\"\"Loads protein data from CSV and directory.\"\"\"\n",
    "#     seqs = pd.read_csv(csv_file)\n",
    "#     protein_data = {}\n",
    "#     for filename in os.listdir(train_dir):\n",
    "#         if filename.endswith(\".csv\"):\n",
    "#             protein_id = re.split(r'_train|_test', filename)[0]\n",
    "#             protein_data[protein_id] = pd.read_csv(os.path.join(train_dir, filename))\n",
    "#     return seqs, protein_data\n",
    "\n",
    "# def load_labels(label_file):\n",
    "#     \"\"\"Loads labels from a CSV file.\"\"\"\n",
    "#     if label_file:\n",
    "#         return pd.read_csv(label_file)\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_sequence(sequence):\n",
    "#     \"\"\"Encodes a protein sequence using one-hot encoding.\"\"\"\n",
    "#     encoded_sequence = np.zeros((len(sequence), len(amino_acid_mapping)), dtype=int)\n",
    "#     for i, amino_acid in enumerate(sequence):\n",
    "#         index = amino_acid_mapping.get(amino_acid, amino_acid_mapping['X'])\n",
    "#         encoded_sequence[i, index] = 1\n",
    "#     return encoded_sequence\n",
    "\n",
    "# def normalize_pssm(pssm, normalize_method='min-max'):\n",
    "#     \"\"\"Normalizes a PSSM using the specified method.\"\"\"\n",
    "#     # Assuming the first two columns are non-numeric; adjust as necessary based on your actual data format\n",
    "#     numeric_columns = pssm[:, 2:]  # Adjust this if your numeric data starts from a different column\n",
    "\n",
    "#     # Convert to floats & handle any errors\n",
    "#     try:\n",
    "#         pssm_numeric = numeric_columns.astype(np.float32)\n",
    "#     except ValueError as e:\n",
    "#         raise ValueError(f\"Error converting PSSM to float: {e}\")\n",
    "\n",
    "#     if normalize_method == 'min-max':\n",
    "#         # Min-Max normalization\n",
    "#         pssm_min = pssm_numeric.min(axis=0)\n",
    "#         pssm_max = pssm_numeric.max(axis=0)\n",
    "#         # Ensure no division by zero\n",
    "#         pssm_range = np.where(pssm_max - pssm_min == 0, 1, pssm_max - pssm_min)\n",
    "#         normalized_pssm = (pssm_numeric - pssm_min) / pssm_range\n",
    "#     elif normalize_method == 'z-score':\n",
    "#         # Z-Score normalization\n",
    "#         pssm_mean = pssm_numeric.mean(axis=0)\n",
    "#         pssm_std = pssm_numeric.std(axis=0)\n",
    "#         # Avoid division by zero\n",
    "#         pssm_std = np.where(pssm_std == 0, 1, pssm_std)\n",
    "#         normalized_pssm = (pssm_numeric - pssm_mean) / pssm_std\n",
    "#     else:\n",
    "#         # If no normalization method provided, return the original PSSM\n",
    "#         normalized_pssm = pssm_numeric\n",
    "\n",
    "#     return normalized_pssm\n",
    "\n",
    "\n",
    "# def prepare_data_point(idx, seqs, protein_data, label_file=None):\n",
    "#     \"\"\"Prepares a protein sample for training or inference.\"\"\"\n",
    "#     labels = load_labels(label_file)\n",
    "#     protein_id = seqs.iloc[idx]['PDB_ID']\n",
    "#     sequence = seqs.iloc[idx]['SEQUENCE']\n",
    "#     encoded_sequence = encode_sequence(sequence)  # Encode the sequence\n",
    "#     pssm = protein_data[protein_id].values  # Assuming you will process PSSM separately\n",
    "#     normalized_pssm = normalize_pssm(pssm)  # Ensure this is uncommented to use normalized PSSM\n",
    "\n",
    "#     if labels is not None:\n",
    "#         label_seq = labels.iloc[idx]['SEC_STRUCT']\n",
    "#         label_numeric = [sec_struct_mapping[char] for char in label_seq]\n",
    "#         label_tensor = torch.tensor(label_numeric, dtype=torch.long)\n",
    "#         return (\n",
    "#             protein_id,\n",
    "#             torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "#             torch.tensor(normalized_pssm, dtype=torch.float32),\n",
    "#             label_tensor\n",
    "#         )\n",
    "\n",
    "#     return (\n",
    "#         protein_id,\n",
    "#         torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "#         torch.tensor(normalized_pssm, dtype=torch.float32)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_sequence(sequence, amino_acid_mapping):\n",
    "#     # Convert each amino acid in the sequence to a one-hot encoded vector\n",
    "#     encoded_sequence = np.zeros((len(sequence), len(amino_acid_mapping)), dtype=int)\n",
    "#     for i, amino_acid in enumerate(sequence):\n",
    "#         # Default to 'X' for unknown amino acids\n",
    "#         index = amino_acid_mapping.get(amino_acid, amino_acid_mapping['X'])\n",
    "#         encoded_sequence[i, index] = 1\n",
    "#     return encoded_sequence\n",
    "\n",
    "# def normalize_pssm(pssm, normalize_method='min-max'):\n",
    "#     # Assuming the first two columns are non-numeric; adjust as necessary based on your actual data format\n",
    "#     numeric_columns = pssm[:, 2:]  # Adjust this if your numeric data starts from a different column\n",
    "\n",
    "#     # Convert to floats\n",
    "#     try:\n",
    "#         pssm_numeric = numeric_columns.astype(np.float32)\n",
    "#     except ValueError as e:\n",
    "#         # Handle or log the error if needed\n",
    "#         raise ValueError(f\"Error converting PSSM to float: {e}\")\n",
    "\n",
    "#     if normalize_method == 'min-max':\n",
    "#         # Min-Max normalization\n",
    "#         pssm_min = pssm_numeric.min(axis=0)\n",
    "#         pssm_max = pssm_numeric.max(axis=0)\n",
    "#         # Ensure no division by zero\n",
    "#         pssm_range = np.where(pssm_max - pssm_min == 0, 1, pssm_max - pssm_min)\n",
    "#         normalized_pssm = (pssm_numeric - pssm_min) / pssm_range\n",
    "#     elif normalize_method == 'z-score':\n",
    "#         # Z-Score normalization\n",
    "#         pssm_mean = pssm_numeric.mean(axis=0)\n",
    "#         pssm_std = pssm_numeric.std(axis=0)\n",
    "#         # Avoid division by zero\n",
    "#         pssm_std = np.where(pssm_std == 0, 1, pssm_std)\n",
    "#         normalized_pssm = (pssm_numeric - pssm_mean) / pssm_std\n",
    "#     else:\n",
    "#         # If no normalization method provided, return the original PSSM\n",
    "#         normalized_pssm = pssm_numeric\n",
    "\n",
    "#     return normalized_pssm\n",
    "\n",
    "# def protein_dataset(csv_file, train_dir, label_file=None, normalize_method='min-max'):\n",
    "#     # Load the sequences\n",
    "#     seqs = pd.read_csv(csv_file)\n",
    "\n",
    "#     # Load the protein data from the directory\n",
    "#     protein_data = {}\n",
    "#     for filename in os.listdir(train_dir):\n",
    "#         if filename.endswith(\".csv\"):  # Check if the file is a CSV\n",
    "#             protein_id = re.split(r'_train|_test', filename)[0]\n",
    "#             protein_data[protein_id] = pd.read_csv(os.path.join(train_dir, filename))\n",
    "\n",
    "#     # Load the labels, if provided\n",
    "#     if label_file:\n",
    "#         labels = pd.read_csv(label_file)\n",
    "#     else:\n",
    "#         labels = None\n",
    "\n",
    "#     # Amino acid mapping\n",
    "#     amino_acid_mapping = {\n",
    "#         'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4,\n",
    "#         'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9,\n",
    "#         'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14,\n",
    "#         'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19,\n",
    "#         'X': 20,  # Typically used for unknown amino acids\n",
    "#         'B': 21,  # Asparagine or Aspartic acid\n",
    "#         'Z': 22,  # Glutamine or Glutamic acid\n",
    "#         'J': 23,  # Leucine or Isoleucine\n",
    "#         '-': 24,  # Gap or padding\n",
    "#     }\n",
    "\n",
    "# def get_item(idx):\n",
    "#     protein_id = seqs.iloc[idx]['PDB_ID']\n",
    "#     sequence = seqs.iloc[idx]['SEQUENCE']\n",
    "#     encoded_sequence = encode_sequence(sequence, amino_acid_mapping)  # Encode the sequence\n",
    "#     pssm = protein_data[protein_id].values  # Assuming you will process PSSM separately\n",
    "#     normalized_pssm = normalize_pssm(pssm, normalize_method)  # Ensure this is uncommented to use normalized PSSM\n",
    "\n",
    "#     if labels is not None:\n",
    "#         label_seq = labels.iloc[idx]['SEC_STRUCT']\n",
    "#         label_numeric = [sec_struct_mapping[char] for char in label_seq]\n",
    "#         label_tensor = torch.tensor(label_numeric, dtype=torch.long)\n",
    "#         return (\n",
    "#             protein_id,\n",
    "#             torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "#             torch.tensor(normalized_pssm, dtype=torch.float32),\n",
    "#             label_tensor\n",
    "#         )\n",
    "\n",
    "#     return (\n",
    "#         protein_id,\n",
    "#         torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "#         torch.tensor(normalized_pssm, dtype=torch.float32)\n",
    "#     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_optimizer(optimizer_type, model, lr, weight_decay):\n",
    "#     # Choose the optimizer based on the parameterization\n",
    "#     if optimizer_type == \"adam\":\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#     elif optimizer_type == \"sgd\":\n",
    "#         optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#     elif optimizer_type == \"rmsprop\":\n",
    "#         optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#     else:\n",
    "#         raise ValueError(\"Unknown optimizer\")\n",
    "\n",
    "#     return optimizer\n",
    "\n",
    "\n",
    "# def train_with_params(\n",
    "#         lr=0.001,\n",
    "#         batch_size=4,\n",
    "#         hidden_layers=5,\n",
    "#         dropout_rate=0.233246,\n",
    "#         weight_decay=0.0,\n",
    "#         optimizer='rmsprop',\n",
    "#         normalization='min-max',\n",
    "#         num_epochs=10,\n",
    "#         output_file='submission.csv'\n",
    "# ):\n",
    "#     train_dataset = ProteinDataset(csv_file=seqs_train_path, train_dir=train_path, label_file=labels_train_path,\n",
    "#                                    normalize_method=normalization)\n",
    "#     train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "#     test_dataset = ProteinDataset(csv_file=seqs_test_path, train_dir=test_path, normalize_method=normalization)\n",
    "\n",
    "#     # Splitting train_dataset into train and validation sets (adjust sizes as needed)\n",
    "#     train_size = int(0.8 * len(train_dataset))\n",
    "#     val_size = len(train_dataset) - train_size\n",
    "#     train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "#     val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "#     model = FullyConvolutionalProteinModel(hidden_layers_number=hidden_layers, dropout_rate=dropout_rate)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = get_optimizer(optimizer, model, lr, weight_decay)\n",
    "\n",
    "#     train_model(model, criterion, optimizer, train_dataloader, num_epochs)\n",
    "#     validate_model(model, criterion, val_loader)\n",
    "#     test_model_direct(model, test_dataset, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load training data\n",
    "# seqs_train, protein_data_train = load_protein_data(seqs_train_path, train_path)\n",
    "# labels_train = load_labels(labels_train_path)\n",
    "\n",
    "# # Prepare training samples (assuming collate_fn handles batching and shuffling)\n",
    "# train_samples = []\n",
    "# for idx in range(len(seqs_train)):\n",
    "#     protein_id = seqs_train.iloc[idx]['PDB_ID']\n",
    "#     sequence = seqs_train.iloc[idx]['SEQUENCE']\n",
    "#     pssm = protein_data_train[protein_id].values\n",
    "#     sample = prepare_protein_sample(\n",
    "#         protein_id, sequence, pssm, labels_train and labels_train.iloc[idx],  # Include label if exists\n",
    "#         amino_acid_mapping, normalize_method, sec_struct_mapping\n",
    "#     )\n",
    "#     train_samples.append(sample)\n",
    "\n",
    "# # Load testing data\n",
    "# seqs_test, protein_data_test = load_protein_data(seqs_test_path, test_path)\n",
    "\n",
    "# # Prepare testing samples (assuming collate_fn handles batching)\n",
    "# test_samples = []\n",
    "# for idx in range(len(seqs_test)):\n",
    "#     protein_id = seqs_test.iloc[idx]['PDB_ID']\n",
    "#     sequence = seqs_test.iloc[idx]['SEQUENCE']\n",
    "#     pssm = protein_data_test[protein_id].values\n",
    "#     sample = prepare_protein_sample(\n",
    "#         protein_id, sequence, pssm, labels_train.empty and None or labels_train.iloc[idx],  # Include label if exists\n",
    "#         amino_acid_mapping, normalize_method, sec_struct_mapping\n",
    "#     )\n",
    "\n",
    "#     test_samples.append(sample)\n",
    "\n",
    "# # Create dataloaders (assuming collate_fn remains the same)\n",
    "# train_dataloader = DataLoader(train_samples, batch_size=4, collate_fn=collate_fn)\n",
    "# test_dataloader = DataLoader(test_samples, batch_size=4, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seqs, protein_data = load_protein_data(seqs_train_path, train_path)\n",
    "# train_dataset = [prepare_data_point(idx, seqs, protein_data, labels_train_path) for idx in range(len(seqs))]\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "# test_dataset = [prepare_data_point(idx, seqs, protein_data, label_file=None) for idx in range(len(seqs))]\n",
    "\n",
    "# # Model definition and training...\n",
    "# model = FullyConvolutionalProteinModel()\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0)\n",
    "# num_epochs = 25\n",
    "\n",
    "# # Train and Test model on test dataset and create submission file\n",
    "# train_model(model, criterion, optimizer, train_dataloader, num_epochs)\n",
    "# test_model_direct(model, test_dataset, output_file='./data/submission.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# until here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [307, 25] at entry 0 and [142, 25] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 244\u001b[0m\n\u001b[1;32m    241\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mRMSprop(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m    242\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m--> 244\u001b[0m train_model(model, criterion, optimizer, train_dataloader)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Test model on test dataset and create submission file\u001b[39;00m\n\u001b[1;32m    247\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m create_dataloader(seqs_test_path, test_path)\n",
      "Cell \u001b[0;32mIn[5], line 185\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, train_dataloader, num_epochs)\u001b[0m\n\u001b[1;32m    182\u001b[0m correct_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    183\u001b[0m total_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sequences, pssms, labels, _ \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m    186\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m pssms\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Adjust for PSSM data\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [307, 25] at entry 0 and [142, 25] at entry 1"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import re\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# # Define store file paths\n",
    "# DATA_PATH = \"./data/\"\n",
    "# labels_train_path = DATA_PATH + \"labels_train.csv\"\n",
    "# sample_path = DATA_PATH + \"sample.csv\"\n",
    "# seqs_test_path = DATA_PATH + \"seqs_test.csv\"\n",
    "# seqs_train_path = DATA_PATH + \"seqs_train.csv\"\n",
    "# train_path = DATA_PATH + \"train\"\n",
    "# test_path = DATA_PATH + \"test\"\n",
    "\n",
    "# # Define a mapping from amino acid characters to integers\n",
    "# amino_acid_mapping = {\n",
    "#     'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4,\n",
    "#     'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9,\n",
    "#     'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14,\n",
    "#     'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19,\n",
    "#     'X': 20,  # Typically used for unknown amino acids\n",
    "#     'B': 21,  # Asparagine or Aspartic acid\n",
    "#     'Z': 22,  # Glutamine or Glutamic acid\n",
    "#     'J': 23,  # Leucine or Isoleucine\n",
    "#     '-': 24,  # Gap or padding\n",
    "# }\n",
    "\n",
    "# sec_struct_mapping = {'H': 0, 'E': 1, 'C': 2}  # Add more mappings if there are more labels\n",
    "\n",
    "\n",
    "# def load_data(csv_file, train_dir):\n",
    "#     # Load the sequences\n",
    "#     seqs = pd.read_csv(csv_file)\n",
    "\n",
    "#     # Load the protein data from the directory\n",
    "#     protein_data = {}\n",
    "#     for filename in os.listdir(train_dir):\n",
    "#         if filename.endswith(\".csv\"):  # Check if the file is a CSV\n",
    "#             protein_id = re.split(r'_train|_test', filename)[0]\n",
    "#             protein_data[protein_id] = pd.read_csv(os.path.join(train_dir, filename))\n",
    "\n",
    "#     return seqs, protein_data\n",
    "\n",
    "\n",
    "# def encode_sequence(sequence):\n",
    "#     # Convert each amino acid in the sequence to a one-hot encoded vector\n",
    "#     encoded_sequence = np.zeros((len(sequence), len(amino_acid_mapping)), dtype=int)\n",
    "#     for i, amino_acid in enumerate(sequence):\n",
    "#         # Default to 'X' for unknown amino acids\n",
    "#         index = amino_acid_mapping.get(amino_acid, amino_acid_mapping['X'])\n",
    "#         encoded_sequence[i, index] = 1\n",
    "#     return encoded_sequence\n",
    "\n",
    "\n",
    "# def normalize_pssm(pssm, normalize_method='min-max'):\n",
    "#     # Assuming the first two columns are non-numeric; adjust as necessary based on your actual data format\n",
    "#     numeric_columns = pssm[:, 2:]  # Adjust this if your numeric data starts from a different column\n",
    "\n",
    "#     # Convert to floats\n",
    "#     try:\n",
    "#         pssm_numeric = numeric_columns.astype(np.float32)\n",
    "#     except ValueError as e:\n",
    "#         # Handle or log the error if needed\n",
    "#         raise ValueError(f\"Error converting PSSM to float: {e}\")\n",
    "\n",
    "#     if normalize_method == 'min-max':\n",
    "#         # Min-Max normalization\n",
    "#         pssm_min = pssm_numeric.min(axis=0)\n",
    "#         pssm_max = pssm_numeric.max(axis=0)\n",
    "#         # Ensure no division by zero\n",
    "#         pssm_range = np.where(pssm_max - pssm_min == 0, 1, pssm_max - pssm_min)\n",
    "#         normalized_pssm = (pssm_numeric - pssm_min) / pssm_range\n",
    "#     elif normalize_method == 'z-score':\n",
    "#         # Z-Score normalization\n",
    "#         pssm_mean = pssm_numeric.mean(axis=0)\n",
    "#         pssm_std = pssm_numeric.std(axis=0)\n",
    "#         # Avoid division by zero\n",
    "#         pssm_std = np.where(pssm_std == 0, 1, pssm_std)\n",
    "#         normalized_pssm = (pssm_numeric - pssm_mean) / pssm_std\n",
    "#     else:\n",
    "#         # If no normalization method provided, return the original PSSM\n",
    "#         normalized_pssm = pssm_numeric\n",
    "\n",
    "#     return normalized_pssm\n",
    "\n",
    "\n",
    "# def prepare_data_point(idx, seqs, protein_data, label_file=None):\n",
    "#     # Load the labels, if provided\n",
    "#     if label_file:\n",
    "#         labels = pd.read_csv(label_file)\n",
    "#     else:\n",
    "#         labels = None\n",
    "\n",
    "#     protein_id = seqs.iloc[idx]['PDB_ID']\n",
    "#     sequence = seqs.iloc[idx]['SEQUENCE']\n",
    "#     encoded_sequence = encode_sequence(sequence)  # Encode the sequence\n",
    "#     pssm = protein_data[protein_id].values  # Assuming you will process PSSM separately\n",
    "#     normalized_pssm = normalize_pssm(pssm)  # Ensure this is uncommented to use normalized PSSM\n",
    "\n",
    "#     if labels is not None:\n",
    "#         label_seq = labels.iloc[idx]['SEC_STRUCT']\n",
    "#         label_numeric = [sec_struct_mapping[char] for char in label_seq]\n",
    "#         label_tensor = torch.tensor(label_numeric, dtype=torch.long)\n",
    "#         return (\n",
    "#             protein_id,\n",
    "#             torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "#             torch.tensor(normalized_pssm, dtype=torch.float32),\n",
    "#             label_tensor\n",
    "#         )\n",
    "\n",
    "#     return (\n",
    "#         protein_id,\n",
    "#         torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "#         torch.tensor(normalized_pssm, dtype=torch.float32)\n",
    "#     )\n",
    "\n",
    "\n",
    "# def create_dataloader(csv_file, train_dir, label_file=None, batch_size=4):\n",
    "#     seqs, protein_data = load_data(csv_file, train_dir)\n",
    "#     data = [prepare_data_point(idx, seqs, protein_data, label_file) for idx in range(len(seqs))]\n",
    "#     return DataLoader(data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     _, sequences, pssms, labels_list = zip(*batch)  # Unzip the batch\n",
    "\n",
    "#     # Pad sequences and PSSMs\n",
    "#     sequences_padded = pad_sequence([seq.clone().detach() for seq in sequences], batch_first=True)\n",
    "\n",
    "#     pssms_padded = pad_sequence([pssm.clone().detach() for pssm in pssms], batch_first=True)\n",
    "\n",
    "#     # Handling labels correctly\n",
    "#     if labels_list[0] is not None:  # Check if labels exist\n",
    "#         labels_padded = pad_sequence([label.clone().detach() for label in labels_list], batch_first=True)\n",
    "\n",
    "#     else:\n",
    "#         labels_padded = None\n",
    "\n",
    "#     # Create a mask based on the original sequence lengths\n",
    "#     mask = [torch.ones(len(label), dtype=torch.uint8) for label in labels_list]\n",
    "#     mask_padded = pad_sequence(mask, batch_first=True, padding_value=0)  # Assuming padding_value for labels is 0\n",
    "#     return sequences_padded, pssms_padded, labels_padded, mask_padded\n",
    "\n",
    "\n",
    "# class FullyConvolutionalProteinModel(torch.nn.Module):\n",
    "#     def __init__(self, num_classes=3, input_channels=20):  # 20 for amino acid one-hot, adjust if using PSSM\n",
    "#         super(FullyConvolutionalProteinModel, self).__init__()\n",
    "\n",
    "#         # Define convolutional layers\n",
    "#         self.conv1 = torch.nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, padding=1)\n",
    "#         self.conv2 = torch.nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "#         self.conv3 = torch.nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "#         # Final layer that maps to the number of classes\n",
    "#         self.final_conv = torch.nn.Conv1d(in_channels=256, out_channels=num_classes, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Apply convolutional layers with activation functions\n",
    "#         x = torch.nn.functional.relu(self.conv1(x))\n",
    "#         x = torch.nn.functional.relu(self.conv2(x))\n",
    "#         x = torch.nn.functional.relu(self.conv3(x))\n",
    "\n",
    "#         # Apply final convolutional layer - no activation, as CrossEntropyLoss includes it\n",
    "#         x = self.final_conv(x)\n",
    "\n",
    "#         # No softmax here, as nn.CrossEntropyLoss applies it internally.\n",
    "#         # Transpose the output to match [batch_size, sequence_length, num_classes]\n",
    "#         # This makes it easier to calculate loss later\n",
    "#         x = x.transpose(1, 2)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "# def train_model(model, criterion, optimizer, train_dataloader, num_epochs=10):\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()  # Set model to training mode\n",
    "#         running_loss = 0.0\n",
    "#         correct_preds = 0\n",
    "#         total_preds = 0\n",
    "\n",
    "#         for sequences, pssms, labels, _ in train_dataloader:\n",
    "#             inputs = pssms.permute(0, 2, 1)  # Adjust for PSSM data\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs.transpose(1, 2), labels)\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "#             # Calculate training accuracy\n",
    "#             _, predicted = torch.max(outputs, 2)  # Get the index of the max log-probability\n",
    "#             correct_preds += (predicted == labels).sum().item()\n",
    "#             total_preds += labels.numel()\n",
    "\n",
    "#         epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "#         epoch_acc = correct_preds / total_preds\n",
    "#         print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "\n",
    "# def test_model_direct(model, test_dataset, output_file='./submission.csv'):\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     predictions = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for i in range(len(test_dataset)):  # Iterate directly over the dataset\n",
    "#             pdb_id, _, pssm = test_dataset[i]  # Assuming the dataset returns PDB_ID, sequence, and PSSM\n",
    "\n",
    "#             # Prepare the input tensor; add an extra batch dimension using unsqueeze\n",
    "#             input_pssm = pssm.unsqueeze(0).permute(0, 2, 1)  # Adjust dimensions to [1, features, seq_len]\n",
    "\n",
    "#             # Make a prediction\n",
    "#             outputs = model(input_pssm)\n",
    "#             _, predicted = torch.max(outputs, 2)  # Get the index of max log-probability\n",
    "\n",
    "#             # Process the predictions\n",
    "#             seq_len = pssm.shape[0]  # Assuming pssm is [features, seq_len]\n",
    "#             for j in range(seq_len):\n",
    "#                 residue_id = f\"{pdb_id}_{j + 1}\"  # Construct the ID\n",
    "#                 structure_label = ['H', 'E', 'C'][predicted[0, j].item()]  # Map numeric predictions to labels\n",
    "#                 predictions.append([residue_id, structure_label])\n",
    "\n",
    "#     # Write predictions to CSV\n",
    "#     pd.DataFrame(predictions, columns=['ID', 'STRUCTURE']).to_csv(output_file, index=False)\n",
    "#     print(f'Submission file saved to {output_file}')\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     train_dataloader = create_dataloader(seqs_train_path, train_path, label_file=labels_train_path, batch_size=4)\n",
    "\n",
    "#     # Model definition and training...\n",
    "#     model = FullyConvolutionalProteinModel()\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "#     optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0)\n",
    "#     num_epochs = 10\n",
    "\n",
    "#     train_model(model, criterion, optimizer, train_dataloader)\n",
    "\n",
    "#     # Test model on test dataset and create submission file\n",
    "#     test_dataset = create_dataloader(seqs_test_path, test_path)\n",
    "#     test_model_direct(model, test_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
